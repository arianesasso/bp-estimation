{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Signal Processing\n",
    "import scipy.signal as sig\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier, RandomForestRegressor\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def drop_correlation(df, labels, threshold = 0.95, plotcorr = False):\n",
    "    corr = df.loc[:, ~df.columns.isin(labels)].corr()\n",
    "    if plotcorr: \n",
    "        f, ax = plt.subplots(figsize=(15, 15))\n",
    "        cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "        sns.heatmap(corr, cmap = cmap,\n",
    "                xticklabels=corr.columns.values,\n",
    "                yticklabels=corr.columns.values)\n",
    "    # Select upper triangle of correlation matrix\n",
    "    upper = corr.abs().where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "    # Find features with correlation greater than threshold\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "    print(\"Columns dropped: \", len(to_drop))\n",
    "    # Drop features \n",
    "    df.drop(columns = to_drop, inplace=True)\n",
    "    print(\"New Dataframe Shape: \" + str(df.shape))\n",
    "    return(df)\n",
    "\n",
    "def predict_bp_from_ppg(dataframe, predicted_variable = 'SBP', k = 1, \n",
    "                        correlation_threshold = 0.95,\n",
    "                        random_seed = 42, learning_rate = 0.01, \n",
    "                        n_estimators = 100, \n",
    "                        alpha = 1, l1_ratio = 0.5, random_state = 42, \n",
    "                        epochs = 50, \n",
    "                        batch_size = 5, n_jobs = -1, max_depth = 10, \n",
    "                        verbose = False):    \n",
    "        \n",
    "    df = dataframe.rename(columns={\"SYS(mmHg)\": \"SBP\", \"DIA(mmHg)\": \"DBP\", 'subject': 'patientid'})\n",
    "    \n",
    "    # Dropping correlation\n",
    "    df.drop(df.loc[(df['SBP'] == 0)|(df['DBP'] == 0)].index, inplace = True)\n",
    "    df = drop_correlation(df, ['SBP', 'DBP'], correlation_threshold, plotcorr = False)\n",
    "    if verbose: print(df.shape)\n",
    "\n",
    "    features = df.shape[1]-3\n",
    "    print('Nr of features: ', features)\n",
    "    patient_ids = np.unique(df['patientid'])\n",
    "    \n",
    "    # Create estimators\n",
    "    estimators_lr = []\n",
    "    estimators_gbm = []\n",
    "    estimators_lgbm = []\n",
    "    estimators_rf = []\n",
    "\n",
    "    estimators_lr.append(('standardize', StandardScaler()))\n",
    "    estimators_lr.append(('lr', ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=random_state)))\n",
    "\n",
    "    estimators_gbm.append(('standardize', StandardScaler()))\n",
    "    estimators_gbm.append(('gbm', GradientBoostingRegressor(learning_rate=learning_rate, n_estimators=n_estimators, random_state=random_seed)))\n",
    "    \n",
    "    estimators_lgbm.append(('standardize', StandardScaler()))\n",
    "    estimators_lgbm.append(('lgbm', lgb.LGBMRegressor(learning_rate=learning_rate, n_estimators=n_estimators, random_state=random_seed, n_jobs=n_jobs)))\n",
    "    \n",
    "\n",
    "    estimators_rf.append(('standardize', StandardScaler()))\n",
    "    estimators_rf.append(('rf', RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, random_state=random_state, n_jobs=n_jobs)))\n",
    "            \n",
    "    pipeline_lr = Pipeline(estimators_lr)\n",
    "    pipeline_gbm = Pipeline(estimators_gbm)\n",
    "    pipeline_lgbm = Pipeline(estimators_lgbm)\n",
    "    pipeline_rf = Pipeline(estimators_rf)\n",
    "\n",
    "    RMSE_LR = []\n",
    "    MAPE_LR = []\n",
    "    MAE_LR = []\n",
    "\n",
    "    RMSE_GBM = []\n",
    "    MAPE_GBM = []\n",
    "    MAE_GBM = []\n",
    "    \n",
    "    RMSE_DUMMY = []\n",
    "    MAPE_DUMMY = []\n",
    "    MAE_DUMMY = []\n",
    "    \n",
    "    RMSE_LGBM = []\n",
    "    MAPE_LGBM = []\n",
    "    MAE_LGBM = []\n",
    "    \n",
    "    RMSE_RF = []\n",
    "    MAPE_RF = []\n",
    "    MAE_RF = []\n",
    "\n",
    "    results = {}\n",
    "    i = 0\n",
    "    mean_train = 0\n",
    "    mean_test = 0\n",
    "    total = len(df.index)\n",
    "    subjects = len(df['patientid'].unique())\n",
    "    \n",
    "    if verbose: print(\"BPPairs: \", total)\n",
    "    if verbose: print(\"Subjects: \", subjects)\n",
    "    if verbose: print(\"\\n\")\n",
    "\n",
    "    while len(patient_ids) > 1:\n",
    "        i= i + 1 \n",
    "\n",
    "        # Random Seed\n",
    "        random.seed(random_seed)\n",
    "        \n",
    "        # Drop Condition\n",
    "        drop_condition = 'any'\n",
    "\n",
    "        patient_test_ids = random.choices(patient_ids, k = k)\n",
    "        patient_ids = [e for e in patient_ids if e not in patient_test_ids]\n",
    "        df_test = df.loc[df['patientid'].isin(patient_test_ids)].dropna(how=drop_condition)\n",
    "        df_train = df[~df['patientid'].isin(patient_test_ids)].dropna(how=drop_condition)\n",
    "        if verbose: print(\"Running fold\" + str(i))\n",
    "        if verbose: print(\"Train: \", df_train.shape)\n",
    "        mean_train += len(df_train.index)\n",
    "        if verbose: print(\"Test: \", df_test.shape)\n",
    "        if verbose: print(\"Total: \", len(df_test.index) + len(df_train.index))\n",
    "        mean_test += len(df_test.index)\n",
    "        if verbose: print(\"\\n\")\n",
    "\n",
    "        cols_dropped = ['patientid']\n",
    "\n",
    "        if predicted_variable == 'SBP':\n",
    "            cols_dropped.append('DBP')\n",
    "        elif predicted_variable == 'DBP':\n",
    "            cols_dropped.append('SBP')\n",
    "        df_train = df_train.drop(columns = cols_dropped)\n",
    "        df_test = df_test.drop(columns = cols_dropped)\n",
    "\n",
    "        #lr\n",
    "        pipeline_lr.fit(X = df_train.loc[:, df_train.columns != predicted_variable].values, \n",
    "                        y = df_train[predicted_variable].values)\n",
    "        predicted_labels = pipeline_lr.predict(df_test.loc[:, df_test.columns != predicted_variable].values)\n",
    "\n",
    "        RMSE_LR.append(np.sqrt(mean_squared_error(df_test[predicted_variable], predicted_labels)))  \n",
    "        MAPE_LR.append(mean_absolute_percentage_error(df_test[predicted_variable], predicted_labels))\n",
    "        MAE_LR.append(mean_absolute_error(df_test[predicted_variable], predicted_labels))\n",
    "\n",
    "        #gbm \n",
    "        pipeline_gbm.fit(X = df_train.loc[:, df_train.columns != predicted_variable].values, \n",
    "                         y = df_train[predicted_variable].values)\n",
    "        predicted_labels = pipeline_gbm.predict(df_test.loc[:, df_test.columns != predicted_variable].values)\n",
    "\n",
    "        RMSE_GBM.append(np.sqrt(mean_squared_error(df_test[predicted_variable], predicted_labels)))  \n",
    "        MAPE_GBM.append(mean_absolute_percentage_error(df_test[predicted_variable], predicted_labels))\n",
    "        MAE_GBM.append(mean_absolute_error(df_test[predicted_variable], predicted_labels))\n",
    "        \n",
    "        #lightgbm\n",
    "        pipeline_lgbm.fit(X = df_train.loc[:, df_train.columns != predicted_variable].values, y = df_train[predicted_variable].values)\n",
    "        predicted_labels = pipeline_lgbm.predict(df_test.loc[:, df_test.columns != predicted_variable].values)\n",
    "\n",
    "        RMSE_LGBM.append(np.sqrt(mean_squared_error(df_test[predicted_variable], predicted_labels)))  \n",
    "        MAPE_LGBM.append(mean_absolute_percentage_error(df_test[predicted_variable], predicted_labels))\n",
    "        MAE_LGBM.append(mean_absolute_error(df_test[predicted_variable], predicted_labels))\n",
    "        \n",
    "        #rf\n",
    "        pipeline_rf.fit(X = df_train.loc[:, df_train.columns != predicted_variable].values, y = df_train[predicted_variable].values)\n",
    "        predicted_labels = pipeline_rf.predict(df_test.loc[:, df_test.columns != predicted_variable].values)\n",
    "\n",
    "        RMSE_RF.append(np.sqrt(mean_squared_error(df_test[predicted_variable], predicted_labels)))  \n",
    "        MAPE_RF.append(mean_absolute_percentage_error(df_test[predicted_variable], predicted_labels))\n",
    "        MAE_RF.append(mean_absolute_error(df_test[predicted_variable], predicted_labels))\n",
    "        \n",
    "        # dummy_mean\n",
    "        X = df_train.loc[:, df_train.columns != predicted_variable].values\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        dummy_mean = DummyRegressor(strategy='mean')\n",
    "        \n",
    "        dummy_mean.fit(X_scaled, \n",
    "                         y = df_train[predicted_variable].values)\n",
    "        predicted_labels = dummy_mean.predict(df_test.loc[:, df_test.columns != predicted_variable].values)\n",
    "\n",
    "        RMSE_DUMMY.append(np.sqrt(mean_squared_error(df_test[predicted_variable], predicted_labels)))  \n",
    "        MAPE_DUMMY.append(mean_absolute_percentage_error(df_test[predicted_variable], predicted_labels))\n",
    "        MAE_DUMMY.append(mean_absolute_error(df_test[predicted_variable], predicted_labels))\n",
    "    \n",
    "    # General Info\n",
    "    results['subjects'] = subjects\n",
    "    results['bp_pairs'] = total\n",
    "    results['folders'] = i\n",
    "    results['mean_train_size'] = round(mean_train/i)\n",
    "    results['mean_test_size'] = round(mean_test/i)\n",
    "    \n",
    "    # Mean LR\n",
    "    results['RMSE_LR_MEAN'] = np.mean(np.array(RMSE_LR))\n",
    "    results['MAPE_LR_MEAN'] = np.mean(np.array(MAPE_LR))\n",
    "    results['MAE_LR_MEAN'] = np.mean(np.array(MAE_LR))\n",
    "            \n",
    "    # STD LR\n",
    "    results['RMSE_LR_STD'] = np.std(np.array(RMSE_LR))\n",
    "    results['MAPE_LR_STD'] = np.std(np.array(MAPE_LR))\n",
    "    results['MAE_LR_STD'] = np.std(np.array(MAE_LR))\n",
    "\n",
    "    # Mean GBM\n",
    "    results['RMSE_GBM_MEAN'] = np.mean(np.array(RMSE_GBM))\n",
    "    results['MAPE_GBM_MEAN'] = np.mean(np.array(MAPE_GBM))\n",
    "    results['MAE_GBM_MEAN'] = np.mean(np.array(MAE_GBM))\n",
    "    \n",
    "    # Std GBM\n",
    "    results['RMSE_GBM_STD'] = np.std(np.array(RMSE_GBM))\n",
    "    results['MAPE_GBM_STD'] = np.std(np.array(MAPE_GBM))\n",
    "    results['MAE_GBM_STD'] = np.std(np.array(MAE_GBM))\n",
    "    \n",
    "    # Mean LGBM\n",
    "    results['RMSE_LGBM_MEAN'] = np.mean(np.array(RMSE_LGBM))\n",
    "    results['MAPE_LGBM_MEAN'] = np.mean(np.array(MAPE_LGBM))\n",
    "    results['MAE_LGBM_MEAN'] = np.mean(np.array(MAE_LGBM))\n",
    "    \n",
    "    # Std LGBM\n",
    "    results['RMSE_LGBM_STD'] = np.std(np.array(RMSE_LGBM))\n",
    "    results['MAPE_LGBM_STD'] = np.std(np.array(MAPE_LGBM))\n",
    "    results['MAE_LGBM_STD'] = np.std(np.array(MAE_LGBM))\n",
    "    \n",
    "    # Mean RF\n",
    "    results['RMSE_RF_MEAN'] = np.mean(np.array(RMSE_RF))\n",
    "    results['MAPE_RF_MEAN'] = np.mean(np.array(MAPE_RF))\n",
    "    results['MAE_RF_MEAN'] = np.mean(np.array(MAE_RF))\n",
    "    \n",
    "    # Std RF\n",
    "    results['RMSE_RF_STD'] = np.std(np.array(RMSE_RF))\n",
    "    results['MAPE_RF_STD'] = np.std(np.array(MAPE_RF))\n",
    "    results['MAE_RF_STD'] = np.std(np.array(MAE_RF))\n",
    "    \n",
    "    # Mean Dummy\n",
    "    results['RMSE_DUMMY_MEAN'] = np.mean(np.array(RMSE_DUMMY))\n",
    "    results['MAPE_DUMMY_MEAN'] = np.mean(np.array(MAPE_DUMMY))\n",
    "    results['MAE_DUMMY_MEAN'] = np.mean(np.array(MAE_DUMMY))\n",
    "    \n",
    "    # Std Dummy\n",
    "    results['RMSE_DUMMY_STD'] = np.std(np.array(RMSE_DUMMY))\n",
    "    results['MAPE_DUMMY_STD'] = np.std(np.array(MAPE_DUMMY))\n",
    "    results['MAE_DUMMY_STD'] = np.std(np.array(MAE_DUMMY))\n",
    "    \n",
    "    parameters = {\n",
    "                    'predicted_variable' : predicted_variable,\n",
    "                    'correlation_threshold' : correlation_threshold, \n",
    "                    'random_seed' :  random_seed,\n",
    "                    'learning_rate' : learning_rate, \n",
    "                    'n_estimators' : n_estimators, \n",
    "                    'alpha' : alpha, \n",
    "                    'l1_ratio' : l1_ratio,\n",
    "                    'random_state' : random_state, \n",
    "                    'k' : k, \n",
    "                    'features' : features, \n",
    "                    'epochs' : epochs, \n",
    "                    'batch_size' : batch_size,\n",
    "                    'max_depth' : max_depth,\n",
    "                    'n_jobs' : n_jobs,\n",
    "    }    \n",
    "    results.update(parameters)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(myPath):\n",
    "    file_names = [f for f in os.listdir(myPath) if os.path.isfile(os.path.join(myPath, f)) and f[-4:] == \".txt\"]\n",
    "    file_names.sort()\n",
    "    return file_names\n",
    "\n",
    "def apply_filter(df, filter_type='cheby', verbose=False):\n",
    "    if filter_type == 'avg':\n",
    "        df['bvp_filtered'] = pd.DataFrame(np.convolve(df['bvp'], np.ones((100,))/100, mode='valid'))\n",
    "    elif filter_type == 'cheby':\n",
    "        sos = sig.cheby2(4, 20, [0.5, 8], btype='bandpass', fs=1000, output='sos')\n",
    "        df['bvp_filtered'] = sig.sosfiltfilt(sos, df['bvp'])\n",
    "    elif filter_type == 'butter':\n",
    "        sos = sig.butter(4, [0.5, 8], btype='bandpass', fs=1000, output='sos')\n",
    "        df['bvp_filtered'] = sig.sosfiltfilt(sos, df['bvp'])\n",
    "    if verbose:\n",
    "        fig = plt.figure(figsize=(14, 6))\n",
    "        plt.plot(df.index, df['bvp_filtered'])\n",
    "        plt.xlabel('Samples')\n",
    "        plt.ylabel('Scaled Magnitude')\n",
    "        \n",
    "        # Save PPG Filtered\n",
    "        with open('../../config.json') as f:\n",
    "            config = json.load(f)\n",
    "        today = datetime.datetime.today().strftime('%Y-%m-%d')\n",
    "        figure_path = config['figures']\n",
    "        \n",
    "        ppg_figure_path = os.path.join(figure_path, today, 'ppg_liang')\n",
    "        if not os.path.exists(ppg_figure_path):\n",
    "            os.makedirs(ppg_figure_path)\n",
    "        \n",
    "        fig.savefig(os.path.join(ppg_figure_path, 'filter_liang.pdf'), dpi=300, format='pdf')\n",
    "        plt.show()\n",
    "    return df\n",
    "\n",
    "def normalise(df, method='z_score'):\n",
    "    if 'min_max':\n",
    "        df['bvp'] = (df['bvp'] - df['bvp'].min()) / (df['bvp'].max() - df['bvp'].min())\n",
    "    elif 'z_score':\n",
    "        df['bvp'] =  (df['bvp'] - df['bvp'].mean()) / df['bvp'].std()\n",
    "    return df\n",
    "\n",
    "# finds the local minima that correspond to the starts of a cardiac cycle\n",
    "def find_cycle_starts(df, sample_rate=1000):\n",
    "    minima = sig.find_peaks(-df.values, distance=0.7*sample_rate)[0]\n",
    "    return minima\n",
    "\n",
    "# returns the x values for those samples in the signal, that are closest to some given y value\n",
    "def find_xs_for_y(ys, y_val, sys_peak):\n",
    "    diffs = abs(ys - y_val)\n",
    "    x1 = diffs[:sys_peak].idxmin()\n",
    "    x2 = diffs[sys_peak:].idxmin()\n",
    "    return x1, x2\n",
    "\n",
    "# takes a dataframe of calculated features and removes the outliers occurring due to inaccuracies in the signal\n",
    "def clean_window_features_of_outliers(df):\n",
    "    quant = df.quantile(0.8)\n",
    "    for col in df.columns:\n",
    "        if col.find('ts') == -1:\n",
    "            df = df[df[col] < quant[col]*2]\n",
    "    return df\n",
    "\n",
    "def find_clean_cycles_with_template(signal, verbose=False):\n",
    "    initial_cycle_starts = find_cycle_starts(signal)\n",
    "    if len(initial_cycle_starts) <= 1:\n",
    "        return []\n",
    "    template_length = math.floor(np.median(np.diff(initial_cycle_starts)))\n",
    "    cycle_starts = initial_cycle_starts[:-1]\n",
    "    while cycle_starts[-1] + template_length > len(signal):\n",
    "        cycle_starts = cycle_starts[:-1]\n",
    "    template = []\n",
    "    for i in range(template_length):\n",
    "        template.append(np.mean(signal[cycle_starts + i]))\n",
    "    \n",
    "    corr_coef = []\n",
    "    for cycle_start in cycle_starts:\n",
    "        corr_coef.append(np.corrcoef(template, signal[cycle_start:cycle_start+template_length])[0,1])\n",
    "\n",
    "    valid_indices = np.argwhere(np.array(corr_coef) >= 0.8)\n",
    "    if (len(valid_indices) > len(cycle_starts) / 2) and len(valid_indices) > 1:\n",
    "        cycle_starts = cycle_starts[np.squeeze(valid_indices)]\n",
    "        template2 = []\n",
    "        for i in range(template_length):\n",
    "            template2.append(np.mean(signal[cycle_starts + i]))\n",
    "        template = template2\n",
    "        \n",
    "    if verbose:\n",
    "        print('Cycle Template')\n",
    "        plot = plt.plot(template)\n",
    "        plt.show()\n",
    "        \n",
    "    # Check correlation of cycles with template\n",
    "    # SQI1: Pearson Correlation\n",
    "    sqi1_corr = []\n",
    "    for cycle_start in cycle_starts:\n",
    "        corr, _ = stats.pearsonr(template, np.nan_to_num(signal[cycle_start:cycle_start+template_length]))\n",
    "        sqi1_corr.append(corr)\n",
    "        \n",
    "    # SQI2: Pearson Correlation with \n",
    "    sqi2_corr = []\n",
    "    for cycle_start in cycle_starts:\n",
    "        cycle_end = initial_cycle_starts[np.squeeze(np.argwhere(initial_cycle_starts==cycle_start)) + 1] \n",
    "        corr, _ = stats.pearsonr(template, sig.resample(signal[cycle_start:cycle_end], template_length))\n",
    "        sqi2_corr.append(corr)\n",
    "        \n",
    "    # SQI3: Pearson Correlation\n",
    "    corrs = np.array([sqi1_corr, sqi2_corr]).transpose()\n",
    "    cycle_starts = cycle_starts[np.all(corrs >= 0.8, axis=1)]\n",
    "    \n",
    "    if verbose:\n",
    "        print('Detected Valid Cycles')\n",
    "        fig = plt.figure(figsize=(12, 10), dpi=300)\n",
    "        for cycle_start in cycle_starts:\n",
    "            plt.rcParams.update({'font.size': 16})\n",
    "            plt.plot(signal[cycle_start:cycle_start+template_length].to_numpy())\n",
    "        \n",
    "        # Save Valid Cycles\n",
    "        with open('../../config.json') as f:\n",
    "            config = json.load(f)\n",
    "        today = datetime.datetime.today().strftime('%Y-%m-%d')\n",
    "        figure_path = config['figures']\n",
    "    \n",
    "        millis = int(round(time.time() * 1000))\n",
    "        valid_cycles = os.path.join(figure_path, today, 'valid_cycles_liang')\n",
    "        \n",
    "        if not os.path.exists(valid_cycles):\n",
    "            os.makedirs(valid_cycles)\n",
    "        fig.savefig(os.path.join(valid_cycles, str(millis)+'_valid_cycles_liang.png'))\n",
    "        \n",
    "    cycles = []\n",
    "    for cycle_start in cycle_starts:\n",
    "        cycle_end = initial_cycle_starts[np.squeeze(np.argwhere(initial_cycle_starts==cycle_start)) + 1]\n",
    "        if (cycle_end - cycle_start) > template_length*1.2:\n",
    "            cycle_end = cycle_start + template_length\n",
    "        cycles.append((cycle_start, cycle_end))\n",
    "\n",
    "    return cycles\n",
    "    \n",
    "# Filter PPG Data\n",
    "def extract_features_for_cycle(window_df, signal, verbose=False):\n",
    "    cur_index = window_df.index.max() + 1\n",
    "    if np.isnan(cur_index):\n",
    "        cur_index = 0\n",
    "    signal.index = pd.to_datetime(signal.index, unit='ms')\n",
    "    signal = signal.interpolate(method='time')\n",
    "    signal = signal - signal.min()\n",
    "    max_amplitude = signal.max()\n",
    "    \n",
    "    peaks = sig.find_peaks(signal.values, distance=0.7*1000)[0]\n",
    "    sys_peak_ts = signal.index[peaks[0]]\n",
    "    \n",
    "    if verbose:\n",
    "        # Save Features in Cycle\n",
    "        with open('../../config.json') as f:\n",
    "            config = json.load(f)\n",
    "        today = datetime.datetime.today().strftime('%Y-%m-%d')\n",
    "        figure_path = config['figures']\n",
    "        \n",
    "        features_cycle = os.path.join(figure_path, today, 'features_cycle_liang')\n",
    "        millis = int(round(time.time() * 1000))\n",
    "        \n",
    "        fig = plt.figure(figsize=(12, 10), dpi=300)\n",
    "        plt.rcParams.update({'font.size': 16})\n",
    "        plt.axis('off')\n",
    "        plt.xlim((signal.index.min(), signal.index.max()))\n",
    "        plt.scatter(signal.index[peaks], signal[peaks])\n",
    "        plt.plot(signal.index, signal.values, linewidth=3.0, color=\"black\")\n",
    "        \n",
    "        \n",
    "    # Features\n",
    "    window_df = pd.concat([window_df, pd.DataFrame({'start_ts': signal.index.min(),\n",
    "                                               'sys_peak_ts': sys_peak_ts,\n",
    "                                               'T_S': (sys_peak_ts - signal.index.min()).total_seconds(),\n",
    "                                               'T_D': (signal.index.max() - sys_peak_ts).total_seconds()\n",
    "                                              }, index=[cur_index])], sort=False)\n",
    "    for p in [10, 25, 33, 50, 66, 75]:\n",
    "        p_ampl = p / 100 * max_amplitude\n",
    "        x1, x2 = find_xs_for_y(signal, p_ampl, peaks[0])\n",
    "        if verbose:\n",
    "            plt.scatter([x1, x2], signal[[x1, x2]])\n",
    "        window_df.loc[cur_index, 'DW_'+str(p)] = (x2 - sys_peak_ts).total_seconds()\n",
    "        window_df.loc[cur_index, 'DW_SW_sum_'+str(p)] = (x2 - x1).total_seconds()\n",
    "        window_df.loc[cur_index, 'DW_SW_ratio_'+str(p)] = (x2 - sys_peak_ts) / (sys_peak_ts - x1)\n",
    "    if verbose:\n",
    "        plt.show()\n",
    "        if not os.path.exists(features_cycle):\n",
    "            os.makedirs(features_cycle)\n",
    "        fig.savefig(os.path.join(features_cycle, str(millis)+'_features_cycle_liang.svg'), \n",
    "                    transparent=True, format='svg')\n",
    "    return window_df\n",
    "    \n",
    "def extract_features_for_window(df, verbose=False):\n",
    "    cycles = find_clean_cycles_with_template(df['bvp_filtered'], verbose=verbose)\n",
    "    # All signal peaks\n",
    "    all_peaks = sig.find_peaks(df['bvp_filtered'].values, distance=0.7*1000)[0]\n",
    "    if verbose:\n",
    "        print('All Peaks: ', all_peaks)\n",
    "    if len(cycles) == 0:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    window_features = pd.DataFrame()\n",
    "    cur_index = 0\n",
    "    for i in range(len(cycles)):\n",
    "        window_features = extract_features_for_cycle(window_features, df['bvp_filtered'].iloc[cycles[i][0]:cycles[i][1]], verbose=verbose)\n",
    "        if i > 0:\n",
    "            window_features.loc[cur_index-1, 'CP'] = (window_features.loc[cur_index, 'sys_peak_ts'] - window_features.loc[cur_index-1, 'sys_peak_ts']).total_seconds()\n",
    "        cur_index = cur_index + 1\n",
    "    # Last cycle or only cycle need to relies on the difference between the general peaks\n",
    "    all_peaks_index = len(all_peaks)-1\n",
    "    window_features.loc[cur_index-1, 'CP'] = (all_peaks[all_peaks_index] - all_peaks[all_peaks_index-1])/1000\n",
    "\n",
    "    if verbose:\n",
    "        print('Cycle Features within Window:')\n",
    "        print(window_features)\n",
    "   \n",
    "    # Remove Outliers\n",
    "    window_features = clean_window_features_of_outliers(window_features)\n",
    "    \n",
    "    if verbose:\n",
    "        print('Cycle Features within Window no Outliers:')\n",
    "        print(window_features)\n",
    "    return window_features\n",
    "\n",
    "def extract_features_for_signal(signal, subject_info, verbose=False):   \n",
    "    window_features = extract_features_for_window(signal, verbose)\n",
    "    if window_features.empty:\n",
    "        return pd.DataFrame()\n",
    "    for col in window_features.columns:\n",
    "        if col.find('ts') == -1:\n",
    "            subject_info[col+'_mean'] = window_features[col].mean()\n",
    "            subject_info[col+'_var'] = window_features[col].var()\n",
    "    subject_info.dropna(inplace=True, how='any')\n",
    "    return subject_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(csv=True, time_delta='2.1 sec', time_delta_type = 'bfill', \n",
    "                     experiment_type='liang', motion_filter=False, special_filter='cheby', verbose=False):\n",
    "    \n",
    "    with open('../../config.json') as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    exp_base_path = config['liang']\n",
    "    print(exp_base_path)\n",
    "    figure_path = config['figures']\n",
    "    \n",
    "    if verbose:\n",
    "        print(exp_base_path)\n",
    "        print(figure_path)\n",
    "        print('\\n')\n",
    "\n",
    "    today = datetime.datetime.today().strftime('%Y-%m-%d')\n",
    "    all_features = pd.DataFrame()\n",
    "    subject_files = get_files(exp_base_path)\n",
    "\n",
    "    # Create features path if it does not exist\n",
    "    features_path = os.path.join('..', '..', 'features', 'liang', today, experiment_type.replace(' ',''), time_delta.replace(' ','')+'-'+time_delta_type+'-'+str(special_filter).lower(), 'motion-not-filtered')\n",
    "\n",
    "    if not os.path.exists(features_path):\n",
    "        os.makedirs(features_path)\n",
    "\n",
    "    print('Extracting Features...')\n",
    "    print('-----','\\n')\n",
    "    \n",
    "    for subject_file in subject_files:\n",
    "        info = subject_file.split('_')\n",
    "        subject = info[0]\n",
    "        segment = info[1][:-4]\n",
    "        subject_file_name = os.path.join(exp_base_path, subject_file)\n",
    "        \n",
    "        print(\"Subjec: \", subject)\n",
    "        print(\"Segment: \", segment)\n",
    "        \n",
    "        subject_data = pd.read_csv(subject_file_name, lineterminator='\\t', header=None, names=['bvp'])\n",
    "        if verbose:\n",
    "            subject_data['bvp'].plot(figsize=(10, 10))\n",
    "            plt.show()\n",
    "        \n",
    "        # Subject and Segment\n",
    "        subject_info = pd.DataFrame(data={'subject': [subject], 'segment': [segment]})\n",
    "        normalized_data = normalise(subject_data, 'min_max')\n",
    "        \n",
    "        if verbose:\n",
    "            fig = plt.figure(figsize=(14, 6))\n",
    "            plt.plot(subject_data.index, subject_data['bvp'])\n",
    "            plt.xlabel('Samples')\n",
    "            plt.ylabel('Normalized Magnitude')\n",
    "\n",
    "            ppg_figure_path = os.path.join(figure_path, today, 'ppg_liang')\n",
    "            if not os.path.exists(ppg_figure_path):\n",
    "                os.makedirs(ppg_figure_path)\n",
    "            \n",
    "            fig.savefig(os.path.join(ppg_figure_path, 'normal_liang.pdf'), dpi=300, format='pdf')\n",
    "            plt.show()\n",
    "        \n",
    "        # Apply filter, e. g. moving average, cheby, butter\n",
    "        if special_filter:\n",
    "            filtered_data = apply_filter(normalized_data, special_filter, verbose=verbose)\n",
    "        else:\n",
    "            if verbose: print(\"Not filtered.\")\n",
    "       \n",
    "        features = extract_features_for_signal(filtered_data, subject_info, verbose=verbose)\n",
    "        \n",
    "        print('Features: ', features.shape)\n",
    "\n",
    "        if features.empty:\n",
    "            print('-----','\\n')\n",
    "            continue\n",
    "            \n",
    "        if csv:\n",
    "            features.to_csv(features_path+'/features_{}_{}_{}_{}.csv'.format(subject, segment, experiment_type.replace(' ',''), time_delta.replace(' ','')+'-'+time_delta_type+'-'+str(special_filter).lower()), index=False)\n",
    "        if verbose: \n",
    "            display(features)\n",
    "\n",
    "        if all_features.empty:\n",
    "            all_features = features\n",
    "        else:\n",
    "            all_features = pd.concat([all_features, features])\n",
    "        print('-----','\\n')\n",
    "#         break\n",
    "        \n",
    "    all_features['time_delta'] = time_delta\n",
    "    all_features['time_delta_type'] = time_delta_type\n",
    "    all_features['experiment_type'] = experiment_type\n",
    "    all_features['motion_filter'] = motion_filter\n",
    "    all_features['special_filter'] = special_filter\n",
    "    \n",
    "    # Adding sorting by subject\n",
    "    if 'subject' in all_features:\n",
    "        all_features['subject'] = all_features['subject'].astype(int)\n",
    "        all_features.sort_values(by=['subject','segment'], inplace=True, ignore_index=True)\n",
    "        \n",
    "    if csv:\n",
    "        all_features_path = features_path+'/all_features_{}_{}.csv'.format(experiment_type.replace(' ',''), time_delta.replace(' ','')+'-'+time_delta_type+'-'+str(special_filter).lower())\n",
    "        all_features.to_csv(all_features_path, index=False)\n",
    "    \n",
    "    print('Amount of BP-Pairs: ', all_features.shape)\n",
    "    print('Features Extracted.')\n",
    "    print('-----','\\n')\n",
    "    \n",
    "    if csv:\n",
    "        return all_features_path\n",
    "    else:\n",
    "        return all_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_parameters = {\n",
    "                        'csv' : True,\n",
    "                        'time_delta' : '2.1 sec',\n",
    "                        'time_delta_type': 'ffill',\n",
    "                        'experiment_type' : 'liang',\n",
    "                        'special_filter' : 'cheby',\n",
    "                        'verbose' : False\n",
    "                        }\n",
    "\n",
    "path = extract_features(csv=feature_parameters['csv'],\n",
    "                        time_delta=feature_parameters['time_delta'], \n",
    "                        time_delta_type=feature_parameters['time_delta_type'], \n",
    "                        experiment_type = feature_parameters['experiment_type'], \n",
    "                        special_filter = feature_parameters['special_filter'], \n",
    "                        verbose=feature_parameters['verbose'])\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# > The following experiments were not included in the JAIME 2021 paper, but feel free to play around!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read All Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_delta = '2.1 sec'\n",
    "time_delta_type = 'ffill'\n",
    "experiment_type = 'liang'\n",
    "special_filters = ['cheby']\n",
    "\n",
    "all_paths = []\n",
    "date = datetime.datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "for special_filter in special_filters:\n",
    "    features_path = os.path.join('..', '..', 'features', 'liang', date, experiment_type.replace(' ',''), time_delta.replace(' ','')+'-'+time_delta_type+'-'+str(special_filter).lower())\n",
    "    motion_path = os.path.join(features_path, 'motion-not-filtered')\n",
    "    path = os.path.join(motion_path,'all_features_{}_{}.csv'.format(experiment_type.replace(' ',''), time_delta.replace(' ','')+'-'+time_delta_type+'-'+str(special_filter).lower()))\n",
    "    all_paths.append(path)\n",
    "print(all_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join BP Values and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../config.json') as f:\n",
    "        config = json.load(f)\n",
    "        \n",
    "experiments = config['liang']\n",
    "patient_info = os.path.join(experiments[:-5], 'PPG-BP dataset.xlsx')\n",
    "\n",
    "print(patient_info)\n",
    "patients = pd.read_excel(patient_info, skiprows=1, engine='openpyxl',).rename(\n",
    "    columns={\"Systolic Blood Pressure(mmHg)\":'SYS(mmHg)', 'subject_ID':'subject',\n",
    "            \"Diastolic Blood Pressure(mmHg)\":'DIA(mmHg)',\"Hypertension\" : \"class\"})\n",
    "patients = patients.replace({'class': \n",
    "                             {'Stage 2 hypertension' : 1.0, 'Stage 1 hypertension': 1.0, \n",
    "                              'Prehypertension' : 0.0, 'Normal' : 0.0}})\n",
    "patient_subset = patients[['SYS(mmHg)','DIA(mmHg)','subject','class']]\n",
    "patient_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_subset.groupby('class').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(all_paths[0])\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop Lines with Empty Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(axis=0, how='any', inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_path = all_paths[0][:-len(all_paths[0].split('/').pop())]\n",
    "\n",
    "patients = patient_subset.join(df.set_index('subject'), on='subject', how='inner')\n",
    "print(patients.shape)\n",
    "display(patients.head(5))\n",
    "patients.to_csv(new_path+'/all_patients_with_bp.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = []\n",
    "\n",
    "df = patients.copy()\n",
    "print(\"Complete: \", df.shape)\n",
    "\n",
    "df_hype = df.loc[patients['class'] == 1.0].copy()\n",
    "df_hype.loc[:,'experiment_type'] = 'liang_hype'\n",
    "print(\"Hype: \", df_hype.shape)\n",
    "\n",
    "df_normo = df.loc[patients['class'] == 0.0].copy()\n",
    "df_normo.loc[:,'experiment_type'] = 'liang_normo'\n",
    "print(\"Normo: \", df_normo.shape)\n",
    "\n",
    "if not df.empty:\n",
    "        predicted_variables = ['SBP', 'DBP']\n",
    "        ks = [1]\n",
    "        \n",
    "        features = {\n",
    "                    'time_delta' : df['time_delta'].unique()[0], \n",
    "                    'time_delta_type' : df['time_delta_type'].unique()[0],\n",
    "                    'experiment_type' : df['experiment_type'].unique()[0], \n",
    "                    'motion_filter' : df['motion_filter'].unique()[0],\n",
    "                    'special_filter' : df['special_filter'].unique()[0],\n",
    "                    'segment' : df['segment'].unique()[0],\n",
    "                    'class' : df['segment'].unique()[0]\n",
    "                    }\n",
    "        \n",
    "        df.drop(features.keys(), axis=1, inplace=True)\n",
    "        \n",
    "        features_hype = {\n",
    "                    'time_delta' : df_hype['time_delta'].unique()[0], \n",
    "                    'time_delta_type' : df_hype['time_delta_type'].unique()[0],\n",
    "                    'experiment_type' : df_hype['experiment_type'].unique()[0], \n",
    "                    'motion_filter' : df_hype['motion_filter'].unique()[0],\n",
    "                    'special_filter' : df_hype['special_filter'].unique()[0],\n",
    "                    'segment' : df_hype['segment'].unique()[0],\n",
    "                    'class' : df_hype['segment'].unique()[0]\n",
    "                    }\n",
    "        \n",
    "        df_hype.drop(features_hype.keys(), axis=1, inplace=True)\n",
    "        \n",
    "        features_normo = {\n",
    "                    'time_delta' : df_normo['time_delta'].unique()[0], \n",
    "                    'time_delta_type' : df_normo['time_delta_type'].unique()[0],\n",
    "                    'experiment_type' : df_normo['experiment_type'].unique()[0], \n",
    "                    'motion_filter' : df_normo['motion_filter'].unique()[0],\n",
    "                    'special_filter' : df_normo['special_filter'].unique()[0],\n",
    "                    'segment' : df_normo['segment'].unique()[0],\n",
    "                    'class' : df_normo['segment'].unique()[0]\n",
    "                    }\n",
    "        \n",
    "        df_normo.drop(features_normo.keys(), axis=1, inplace=True)\n",
    "       \n",
    "        for variable in predicted_variables:\n",
    "            for k in ks:\n",
    "                df_total = df\n",
    "                results = predict_bp_from_ppg(df_total, predicted_variable=variable, k=k, \n",
    "                                              correlation_threshold = 0.9, verbose=False)\n",
    "                results.update(features)\n",
    "                experiments.append(results)\n",
    "                \n",
    "                # Hypertensive\n",
    "                results_hype = predict_bp_from_ppg(df_hype, predicted_variable=variable, k=k, \n",
    "                                              correlation_threshold = 0.6, verbose=False)\n",
    "                results_hype.update(features_hype)\n",
    "                experiments.append(results_hype)\n",
    "                \n",
    "                # Normotensive + Prehype\n",
    "                results_normo = predict_bp_from_ppg(df_normo, predicted_variable=variable, k=k, \n",
    "                                              correlation_threshold = 0.75, verbose=False)\n",
    "                results_normo.update(features_normo)\n",
    "                experiments.append(results_normo)\n",
    "                \n",
    "else:\n",
    "    (\"Dataframe was empty: \", path)\n",
    "all_experiments = pd.DataFrame.from_dict(experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_experiments = all_experiments.replace({'motion_filter': {True : 'yes', False: 'no'}})\n",
    "all_experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_experiments[['experiment_type','predicted_variable','MAE_LR_MEAN','MAE_GBM_MEAN','MAE_LGBM_MEAN','MAE_RF_MEAN']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = datetime.datetime.today().strftime('%Y-%m-%d')\n",
    "total = all_experiments.loc[all_experiments['experiment_type'] == 'liang']\n",
    "print(total.shape)\n",
    "total.to_csv('../../results/'+date+'_results_liang.csv', index=True, mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Experiments From Results File and Exploring the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = datetime.datetime.today().strftime('%Y-%m-%d')\n",
    "results_path = '../../results/'+date+'_results_liang.csv'\n",
    "all_experiments = pd.read_csv(results_path)\n",
    "all_experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm = all_experiments[['predicted_variable','MAE_GBM_MEAN','MAE_GBM_STD','MAPE_GBM_MEAN','RMSE_GBM_MEAN','k']].sort_values(by=['predicted_variable','k','MAE_GBM_MEAN'])\n",
    "lr = all_experiments[['predicted_variable','MAE_LR_MEAN','MAE_LR_STD','MAPE_LR_MEAN','RMSE_LR_MEAN','k']].sort_values(by=['predicted_variable','k','MAE_LR_MEAN'])\n",
    "lgbm = all_experiments[['predicted_variable','MAE_LGBM_MEAN','MAE_LGBM_STD','MAPE_LGBM_MEAN','RMSE_LGBM_MEAN','k']].sort_values(by=['predicted_variable','k','MAE_LGBM_MEAN'])\n",
    "rf = all_experiments[['predicted_variable','MAE_RF_MEAN','MAE_RF_STD','MAPE_RF_MEAN','RMSE_RF_MEAN','k']].sort_values(by=['predicted_variable','k','MAE_RF_MEAN'])\n",
    "d = all_experiments[['predicted_variable','experiment_type','MAE_DUMMY_MEAN','MAE_DUMMY_STD','MAPE_DUMMY_MEAN','RMSE_DUMMY_MEAN','special_filter','time_delta','time_delta_type','motion_filter','k']].sort_values(by=['predicted_variable','experiment_type','k','MAE_DUMMY_MEAN'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min liang dbp\n",
    "gbm_min_liang_dbp = gbm.loc[(gbm['predicted_variable'] == 'DBP') & (gbm['k'] == 1)].head(1)[\n",
    "    ['predicted_variable','k','MAE_GBM_MEAN','MAE_GBM_STD']]\n",
    "lgbm_min_liang_dbp = lgbm.loc[(lgbm['predicted_variable'] == 'DBP') & (lgbm['k'] == 1)].head(1)[\n",
    "    ['predicted_variable','k','MAE_LGBM_MEAN','MAE_LGBM_STD']]\n",
    "rf_min_liang_dbp = rf.loc[(rf['predicted_variable'] == 'DBP') & (rf['k'] == 1)].head(1)[\n",
    "    ['predicted_variable','k','MAE_RF_MEAN','MAE_RF_STD']]\n",
    "lr_min_liang_dbp = lr.loc[(lr['predicted_variable'] == 'DBP') & (lr['k'] == 1)].head(1)[\n",
    "    ['predicted_variable','k','MAE_LR_MEAN','MAE_LR_STD']]\n",
    "dummy_min_liang_dbp = d.loc[(d['predicted_variable'] == 'DBP') & (d['k'] == 1)].head(1)[\n",
    "    ['predicted_variable','k','MAE_DUMMY_MEAN','MAE_DUMMY_STD']]\n",
    "\n",
    "min_liang_dbp = gbm_min_liang_dbp.set_index(['predicted_variable','k']).join(lgbm_min_liang_dbp.set_index(\n",
    "    ['predicted_variable','k']), on=['predicted_variable','k'])\n",
    "min_liang_dbp = min_liang_dbp.join(rf_min_liang_dbp.set_index(['predicted_variable','k']), \n",
    "                                   on=['predicted_variable','k'])\n",
    "min_liang_dbp = min_liang_dbp.join(lr_min_liang_dbp.set_index(['predicted_variable','k']), \n",
    "                                   on=['predicted_variable','k'])\n",
    "min_liang_dbp = min_liang_dbp.join(dummy_min_liang_dbp.set_index(['predicted_variable','k']), \n",
    "                                   on=['predicted_variable','k'])\n",
    "\n",
    "# min liang sbp\n",
    "gbm_min_liang_sbp = gbm.loc[(gbm['predicted_variable'] == 'SBP') & (gbm['k'] == 1)].head(1)[\n",
    "    ['predicted_variable','k','MAE_GBM_MEAN','MAE_GBM_STD']]\n",
    "lgbm_min_liang_sbp = lgbm.loc[(lgbm['predicted_variable'] == 'SBP') & (lgbm['k'] == 1)].head(1)[\n",
    "    ['predicted_variable','k','MAE_LGBM_MEAN','MAE_LGBM_STD']]\n",
    "rf_min_liang_sbp = rf.loc[(rf['predicted_variable'] == 'SBP') & (rf['k'] == 1)].head(1)[\n",
    "    ['predicted_variable','k','MAE_RF_MEAN','MAE_RF_STD']]\n",
    "lr_min_liang_sbp = lr.loc[(lr['predicted_variable'] == 'SBP') & (lr['k'] == 1)].head(1)[\n",
    "    ['predicted_variable','k','MAE_LR_MEAN','MAE_LR_STD']]\n",
    "dummy_min_liang_sbp = d.loc[(d['predicted_variable'] == 'SBP') & (d['k'] == 1)].head(1)[\n",
    "    ['predicted_variable','k','MAE_DUMMY_MEAN','MAE_DUMMY_STD']]\n",
    "\n",
    "min_liang_sbp = gbm_min_liang_sbp.set_index(['predicted_variable','k']).join(lgbm_min_liang_sbp.set_index(\n",
    "    ['predicted_variable','k']), on=['predicted_variable','k'])\n",
    "min_liang_sbp = min_liang_sbp.join(rf_min_liang_sbp.set_index(['predicted_variable','k']), \n",
    "                                   on=['predicted_variable','k'])\n",
    "min_liang_sbp = min_liang_sbp.join(lr_min_liang_sbp.set_index(['predicted_variable','k']), \n",
    "                                   on=['predicted_variable','k'])\n",
    "min_liang_sbp = min_liang_sbp.join(dummy_min_liang_sbp.set_index(['predicted_variable','k']), \n",
    "                                   on=['predicted_variable','k'])\n",
    "min_liang_sbp\n",
    "\n",
    "best_results = pd.concat([min_liang_dbp, min_liang_sbp], axis=0)\n",
    "best_results[['MAE_GBM_MEAN','MAE_GBM_STD','MAE_LGBM_MEAN','MAE_LGBM_STD',\n",
    "              'MAE_LR_MEAN','MAE_LR_STD','MAE_DUMMY_MEAN','MAE_DUMMY_STD']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = datetime.datetime.today().strftime('%Y-%m-%d')\n",
    "best_results.to_csv('../../results/'+date+'_best_results_liang.csv', index=True, mode='w')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
