{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import math\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Graphs\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Signal Processing\n",
    "import scipy.stats as stats\n",
    "import scipy.signal as sig\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Custom Package\n",
    "import devicely\n",
    "\n",
    "# Custom Model\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_folders(my_path):\n",
    "    folder_names = [f for f in os.listdir(my_path) if os.path.isdir(os.path.join(my_path, f))]\n",
    "    return folder_names\n",
    "    \n",
    "def truncate_empatica(data_empatica, bp_df, verbose=False):\n",
    "    sub = pd.DataFrame()\n",
    "    for _, row in bp_df.iterrows():\n",
    "        truncated_df = data_empatica.truncate(before=row['window_start'], after=row['window_end'])\n",
    "        if verbose:\n",
    "            truncated_df[['bvp', 'acc_mag']].interpolate(method='index').plot()\n",
    "        sub = sub.append(truncated_df)\n",
    "    if verbose:\n",
    "        print(sub.head())\n",
    "        print(sub.describe())\n",
    "        print(sub.shape)\n",
    "        sub[['bvp', 'acc_mag']].interpolate(method='index').plot()\n",
    "    return sub\n",
    "\n",
    "# finds the local minima that correspond to the start of a cardiac cycle\n",
    "def find_cycle_starts(df, sample_rate=64):\n",
    "    minima = sig.find_peaks(-df.values, distance=0.7*sample_rate)[0]\n",
    "    return minima\n",
    "\n",
    "# returns the x values for those samples in the signal, that are closest to some given y value\n",
    "def find_xs_for_y(ys, y_val, sys_peak):\n",
    "    diffs = abs(ys - y_val)\n",
    "    x1 = diffs[:sys_peak].idxmin()\n",
    "    x2 = diffs[sys_peak:].idxmin()\n",
    "    return x1, x2\n",
    "\n",
    "# takes a dataframe of calculated features and removes the outliers occurring due to inaccuracies in the signal\n",
    "def clean_window_features_of_outliers(df):\n",
    "    quant = df.quantile(0.8)\n",
    "    for col in df.columns:\n",
    "        if col.find('ts') == -1:\n",
    "            df = df[df[col] < quant[col]*2]\n",
    "    return df\n",
    "\n",
    "# finds sections with high acceleration magnitude and removes them\n",
    "def remove_motion_sections(df, limit=100, min_size=5, padding=15, std_mult=0.25):\n",
    "    acc_mag_mean = df['acc_mag'].mean()\n",
    "    acc_mag_std =  df['acc_mag'].std()\n",
    "    # comparison with overall mean and std\n",
    "    thresh_indices = np.squeeze(np.argwhere((df['acc_mag'].values > acc_mag_mean + std_mult * acc_mag_std) | \n",
    "                                            (df['acc_mag'].values < acc_mag_mean - std_mult * acc_mag_std)))\n",
    "\n",
    "    section_indices = []\n",
    "    section_start = thresh_indices[0]\n",
    "    for i in range(1, len(thresh_indices) - 1):\n",
    "        if thresh_indices[i] - thresh_indices[i-1] > limit:\n",
    "            if thresh_indices[i-1] >= section_start + min_size:\n",
    "                section_indices.append((section_start - padding, thresh_indices[i-1] + padding))\n",
    "            section_start = thresh_indices[i]\n",
    "    if thresh_indices[-1] != section_start:\n",
    "        section_indices.append((section_start, thresh_indices[-1]))\n",
    "\n",
    "    section_indices.reverse()\n",
    "    for (start, end) in section_indices:\n",
    "        df = df.drop(index=df.iloc[start:end].index)\n",
    "    return df\n",
    "\n",
    "def find_clean_cycles_with_template(signal, verbose=False):\n",
    "    initial_cycle_starts = find_cycle_starts(signal)\n",
    "    if len(initial_cycle_starts) <= 1:\n",
    "        return []\n",
    "    template_length = math.floor(np.median(np.diff(initial_cycle_starts)))\n",
    "    cycle_starts = initial_cycle_starts[:-1]\n",
    "    while cycle_starts[-1] + template_length > len(signal):\n",
    "        cycle_starts = cycle_starts[:-1]\n",
    "    template = []\n",
    "    for i in range(template_length):\n",
    "        template.append(np.mean(signal[cycle_starts + i]))\n",
    "    \n",
    "    corr_coef = []\n",
    "    for cycle_start in cycle_starts:\n",
    "        corr_coef.append(np.corrcoef(template, signal[cycle_start:cycle_start+template_length])[0,1])\n",
    "\n",
    "    valid_indices = np.argwhere(np.array(corr_coef) >= 0.8)\n",
    "    if (len(valid_indices) > len(cycle_starts) / 2) and len(valid_indices) > 1:\n",
    "        cycle_starts = cycle_starts[np.squeeze(valid_indices)]\n",
    "        template2 = []\n",
    "        for i in range(template_length):\n",
    "            template2.append(np.mean(signal[cycle_starts + i]))\n",
    "        template = template2\n",
    "        \n",
    "    if verbose:\n",
    "        print('Cycle Template')\n",
    "        plt.plot(template)\n",
    "        plt.show()\n",
    "        \n",
    "    # Check correlation of cycles with template\n",
    "    # SQI1: Pearson Correlation\n",
    "    sqi1_corr = []\n",
    "    for cycle_start in cycle_starts:\n",
    "        corr, _ = stats.pearsonr(template, signal[cycle_start:cycle_start+template_length])\n",
    "        sqi1_corr.append(corr)\n",
    "        \n",
    "    # SQI2: Pearson Correlation between the cycle, re-sampled to match the template length, \n",
    "    # and the template itself\n",
    "    sqi2_corr = []\n",
    "    for cycle_start in cycle_starts:\n",
    "        cycle_end = initial_cycle_starts[np.squeeze(np.argwhere(initial_cycle_starts==cycle_start)) + 1] \n",
    "        corr, _ = stats.pearsonr(template, sig.resample(signal[cycle_start:cycle_end], template_length))\n",
    "        sqi2_corr.append(corr)\n",
    "        \n",
    "    # Filter for correlation >= 0.8\n",
    "    corrs = np.array([sqi1_corr, sqi2_corr]).transpose()\n",
    "    cycle_starts = cycle_starts[np.all(corrs >= 0.8, axis=1)]\n",
    "    \n",
    "    if verbose:\n",
    "        print('Detected Valid Cycles')\n",
    "        fig = plt.figure(figsize=(12, 10), dpi=300)\n",
    "        for cycle_start in cycle_starts:\n",
    "            plt.rcParams.update({'font.size': 16})\n",
    "            plt.plot(signal[cycle_start:cycle_start+template_length].to_numpy())\n",
    "           \n",
    "        # Save valid cycles\n",
    "        with open('../../config.json') as f:\n",
    "            config = json.load(f)\n",
    "        today = datetime.datetime.today().strftime('%Y-%m-%d')\n",
    "        figure_path = config['figures']\n",
    "    \n",
    "        millis = int(round(time.time() * 1000))\n",
    "        valid_cycles = os.path.join(figure_path, today, 'valid_cycles_hype')\n",
    "        \n",
    "        if not os.path.exists(valid_cycles):\n",
    "            os.makedirs(valid_cycles)\n",
    "        fig.savefig(os.path.join(valid_cycles, str(millis)+'_valid_cycles_hype.png'))\n",
    "        \n",
    "    cycles = []\n",
    "    for cycle_start in cycle_starts:\n",
    "        cycle_end = initial_cycle_starts[np.squeeze(np.argwhere(initial_cycle_starts==cycle_start)) + 1]\n",
    "        if (cycle_end - cycle_start) > template_length*1.2:\n",
    "            cycle_end = cycle_start + template_length\n",
    "        cycles.append((cycle_start, cycle_end))\n",
    "\n",
    "    return cycles\n",
    "\n",
    "# Filter PPG data\n",
    "def extract_features_for_cycle(window_df, signal, verbose=False):\n",
    "    cur_index = window_df.index.max() + 1\n",
    "    if np.isnan(cur_index):\n",
    "        cur_index = 0\n",
    "    signal = signal.resample('ms').nearest(limit=1).interpolate(method='time')\n",
    "    signal = signal - signal.min()\n",
    "    max_amplitude = signal.max()\n",
    "    \n",
    "    peaks = sig.find_peaks(signal.values)[0]\n",
    "    if len(peaks) == 0:\n",
    "        return pd.DataFrame()\n",
    "    sys_peak_ts = signal.index[peaks[0]]\n",
    "    \n",
    "    if verbose:\n",
    "        plt.figure()\n",
    "        plt.xlim((signal.index.min(), signal.index.max()))\n",
    "        plt.scatter(signal.index[peaks], signal[peaks])\n",
    "        plt.plot(signal.index, signal.values)\n",
    "    # Features\n",
    "    window_df = window_df.append(pd.DataFrame({'start_ts': signal.index.min(),\n",
    "                                               'sys_peak_ts': sys_peak_ts,\n",
    "                                               'T_S': (sys_peak_ts - signal.index.min()).total_seconds(),\n",
    "                                               'T_D': (signal.index.max() - sys_peak_ts).total_seconds()\n",
    "                                              }, index=[cur_index]), sort=False)\n",
    "    for p in [10, 25, 33, 50, 66, 75]:\n",
    "        p_ampl = p / 100 * max_amplitude\n",
    "        x1, x2 = find_xs_for_y(signal, p_ampl, peaks[0])\n",
    "        if verbose:\n",
    "            plt.scatter([x1, x2], signal[[x1, x2]])\n",
    "        window_df.loc[cur_index, 'DW_'+str(p)] = (x2 - sys_peak_ts).total_seconds()\n",
    "        window_df.loc[cur_index, 'DW_SW_sum_'+str(p)] = (x2 - x1).total_seconds()\n",
    "        window_df.loc[cur_index, 'DW_SW_ratio_'+str(p)] = (x2 - sys_peak_ts) / (sys_peak_ts - x1)\n",
    "    if verbose:\n",
    "        plt.show()\n",
    "    return window_df\n",
    "    \n",
    "def extract_features_for_window(df, verbose=False):\n",
    "    cycles = find_clean_cycles_with_template(df['bvp_filtered'], verbose=verbose)\n",
    "    if len(cycles) == 0:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    window_features = pd.DataFrame()\n",
    "    cur_index = 0\n",
    "    for i in range(len(cycles)):\n",
    "        window_features = extract_features_for_cycle(window_features, df['bvp_filtered'].iloc[cycles[i][0]:cycles[i][1]], verbose=verbose)\n",
    "        if window_features.empty:\n",
    "            continue\n",
    "        if i > 0:\n",
    "            if (cur_index-1) in window_features.index:\n",
    "                window_features.loc[cur_index-1, 'CP'] = (window_features.loc[cur_index, 'sys_peak_ts'] - window_features.loc[cur_index-1, 'sys_peak_ts']).total_seconds()\n",
    "            else:\n",
    "                window_features.loc[cur_index-1, 'CP'] = None\n",
    "        cur_index = cur_index + 1\n",
    "    if verbose:\n",
    "        print('Cycle Features within Window:')\n",
    "        print(window_features)\n",
    "    window_features = clean_window_features_of_outliers(window_features)\n",
    "    return window_features\n",
    "\n",
    "def apply_filter(df, filter_type='norm', fs=64):\n",
    "    if len(df['bvp']) <= 27:\n",
    "        df['bvp_filtered'] = df['bvp']\n",
    "        return df['bvp_filtered']\n",
    "    elif filter_type == 'norm':\n",
    "        # No smoothing neccessary due to relatively low sampling frequency\n",
    "        df['bvp_filtered'] = (df['bvp'] - df['bvp'].min()) / (df['bvp'].max() - df['bvp'].min())\n",
    "    elif filter_type == 'cheby':\n",
    "        df['bvp'] = (df['bvp'] - df['bvp'].min()) / (df['bvp'].max() - df['bvp'].min())\n",
    "        sos = sig.cheby2(4, 20, [0.5, 8], btype='bandpass', fs=fs, output='sos')\n",
    "        df['bvp_filtered'] = sig.sosfiltfilt(sos, df['bvp'])\n",
    "    elif filter_type == 'butter':\n",
    "        df['bvp'] = (df['bvp'] - df['bvp'].min()) / (df['bvp'].max() - df['bvp'].min())\n",
    "        sos = sig.butter(4, [0.5, 8], btype='bandpass', fs=fs, output='sos')\n",
    "        df['bvp_filtered'] = sig.sosfiltfilt(sos, df['bvp'])\n",
    "    return df\n",
    "\n",
    "def extract_features_for_signal(signal, bp, fs, verbose=False):\n",
    "    for index, row in bp.iterrows():\n",
    "        window_df = signal.truncate(before=row['window_start'], after=row['window_end'])\n",
    "        if window_df.empty or window_df.shape[0] <= fs:\n",
    "            continue\n",
    "        window_features = extract_features_for_window(window_df, verbose)\n",
    "        for col in window_features.columns:\n",
    "            if col.find('ts') == -1:\n",
    "                bp.loc[index, col+'_mean'] = window_features[col].mean()\n",
    "                bp.loc[index, col+'_var'] = window_features[col].var()\n",
    "    bp.dropna(inplace=True, how='any')\n",
    "    return bp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(csv=True, bp_monitor='spacelabs', timeshift=2, \n",
    "                     time_delta='15 seconds', \n",
    "                     time_delta_type='bfill', \n",
    "                     experiment_type='biking', \n",
    "                     motion_filter=False, special_filter='cheby', verbose=False):\n",
    "    \n",
    "    with open('../../config.json') as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    today = datetime.datetime.today().strftime('%Y-%m-%d')\n",
    "    time_delta_modified = time_delta.split(' ')\n",
    "    time_delta_dict = {time_delta_modified[1]: int(time_delta_modified[0])}\n",
    "\n",
    "    exp_base_path = config['hype']\n",
    "    figure_path = config['figures']\n",
    "    \n",
    "    if verbose:\n",
    "        print(exp_base_path)\n",
    "        print(figure_path)\n",
    "        print('\\n')\n",
    "\n",
    "    dates = get_folders(exp_base_path)\n",
    "    all_features = pd.DataFrame()\n",
    "    \n",
    "    # Create features path if it not exists\n",
    "    features_path = os.path.join('..', '..', 'features', 'hype', today, experiment_type.replace(' ',''), time_delta.replace(' ','')+'-'+time_delta_type+'-'+str(special_filter).lower())\n",
    "    # Filter motion\n",
    "    if motion_filter:\n",
    "        motion_path = os.path.join(features_path, 'motion-filtered')\n",
    "    else:\n",
    "        motion_path = os.path.join(features_path, 'motion-not-filtered')\n",
    "\n",
    "    print('Extracting Features: ', today, experiment_type.replace(' ',''), time_delta.replace(' ','')+'-'+time_delta_type+'-'+str(special_filter).lower()+'-motion: '+str(motion_filter))\n",
    "    if not os.path.exists(motion_path):\n",
    "        os.makedirs(motion_path)\n",
    "    \n",
    "    all_features_path = motion_path+'/all_features_{}_{}.csv'.format(experiment_type.replace(' ',''), time_delta.replace(' ','')+'-'+time_delta_type+'-'+str(special_filter).lower())\n",
    "    if (csv == True) and (os.path.exists(all_features_path)):\n",
    "        print('Features previously extracted.')\n",
    "        return all_features_path\n",
    "    \n",
    "    dates.sort()\n",
    "    for date in dates:\n",
    "        print(date)\n",
    "        subjects = get_folders(os.path.join(exp_base_path, date))\n",
    "        subjects = [int(x) for x in subjects]\n",
    "        subjects.sort()\n",
    "        for subject in subjects:\n",
    "            subject = str(subject)\n",
    "            print(\"Subject: \", subject)\n",
    "            if experiment_type == 'biking':\n",
    "                patient_base_path = os.path.join(exp_base_path, date, subject)\n",
    "                tag = os.path.join(patient_base_path, 'Tags')\n",
    "        \n",
    "                # Check for the Tags\n",
    "                if not os.path.exists(tag):\n",
    "                    print('No Tag File.')\n",
    "                    print('-----','\\n')\n",
    "                    continue           \n",
    "            else:\n",
    "                patient_base_path = os.path.join(exp_base_path, date, subject, experiment_type)\n",
    "\n",
    "            # Sources\n",
    "            sources = {\n",
    "                        'empatica' : glob.glob(patient_base_path+r'/Empatica*').pop()\n",
    "                      }\n",
    "\n",
    "            if bp_monitor == 'spacelabs':\n",
    "                sources['spacelabs'] = glob.glob(patient_base_path+r'/*SpaceLabs*').pop()\n",
    "\n",
    "                if not os.path.exists(sources['spacelabs']):\n",
    "                    if verbose: print(\"Subject has no spacelabs file\")\n",
    "                    break\n",
    "                 # Read Spacelabs\n",
    "                for file in os.listdir(sources['spacelabs']):\n",
    "                    if file.endswith(\".abp\"):\n",
    "                        spacelabs_file = os.path.join(sources['spacelabs'], file)\n",
    "                        break\n",
    "                \n",
    "                bp = devicely.SpacelabsReader(spacelabs_file)\n",
    "                bp.drop_EB()\n",
    "                bp.timeshift(pd.Timedelta(-timeshift, unit='H'))\n",
    "                bp.set_window(datetime.timedelta(**time_delta_dict), time_delta_type)\n",
    "                bp.data['subject'] = bp.subject\n",
    "\n",
    "                # Adjust columns\n",
    "                bp_df = bp.data.drop(['error','z','x','y'], axis=1).reset_index().copy()\n",
    "                bp_df = bp_df[bp_df['DIA(mmHg)'] > 10]\n",
    "                print(\"BP values: \", bp_df.shape[0])\n",
    "                      \n",
    "            if verbose: print(bp_df.head(1))\n",
    "\n",
    "            # Read Empatica\n",
    "            empatica = devicely.EmpaticaReader(sources['empatica'])\n",
    "            subset_empatica = empatica.data[['bvp','acc_mag']].dropna(how='all').copy()\n",
    "            if verbose: print(subset_empatica.head(1))\n",
    "\n",
    "            # Truncate data\n",
    "            sub_data_empatica = truncate_empatica(subset_empatica, bp_df, verbose=verbose)\n",
    "            if verbose: print(\"Truncated data: \", sub_data_empatica.shape)\n",
    "\n",
    "            # Apply filters, e. g. normalise, cheby, butter\n",
    "            if special_filter:\n",
    "                sub_data_empatica = apply_filter(sub_data_empatica, special_filter, fs=64)\n",
    "                if verbose: print(\"Filtered [\"+special_filter+\"] mean bvp: \", sub_data_empatica['bvp_filtered'].mean())\n",
    "            else:\n",
    "                sub_data_empatica['bvp_filtered'] = sub_data_empatica['bvp']\n",
    "                if verbose: print(\"Not filtered.\")\n",
    "                    \n",
    "            # Filter motion\n",
    "            if motion_filter:\n",
    "                sub_data_empatica = remove_motion_sections(sub_data_empatica)\n",
    "                if verbose: print(\"Motionless data: \", sub_data_empatica.shape)\n",
    "                motion_path = os.path.join(features_path,'motion-filtered')\n",
    "            else:\n",
    "                motion_path = os.path.join(features_path,'motion-not-filtered')\n",
    "\n",
    "            features = extract_features_for_signal(sub_data_empatica, bp_df, fs=64, verbose=verbose)\n",
    "\n",
    "            if 'T_S_mean' not in features:\n",
    "                print('No features.')\n",
    "                print('-----','\\n')\n",
    "                continue\n",
    "\n",
    "            if verbose: print('Features: ', features.shape)\n",
    "            \n",
    "            if csv:\n",
    "                features.to_csv(motion_path+'/features_{}_{}_{}.csv'.format(subject, experiment_type.replace(' ',''), time_delta.replace(' ','')+'-'+time_delta_type+'-'+str(special_filter).lower()), index=False)\n",
    "            if verbose: print(features)\n",
    "\n",
    "            if all_features.empty:\n",
    "                all_features = features\n",
    "            else:\n",
    "                all_features = all_features.append(features)\n",
    "\n",
    "            if verbose: print('-----','\\n')\n",
    "\n",
    "    all_features['bp_monitor'] = bp_monitor\n",
    "    all_features['timeshift'] = timeshift\n",
    "    all_features['time_delta'] = time_delta\n",
    "    all_features['time_delta_type'] = time_delta_type\n",
    "    all_features['experiment_type'] = experiment_type\n",
    "    all_features['motion_filter'] = motion_filter\n",
    "    all_features['special_filter'] = special_filter\n",
    "    \n",
    "    # Adding sorting by subject\n",
    "    if 'subject' in all_features:\n",
    "        all_features['subject'] = pd.to_numeric(all_features['subject'])\n",
    "        all_features.sort_values(by='subject', inplace=True)   \n",
    "        if csv:\n",
    "            all_features.to_csv(all_features_path, index=False)\n",
    "    \n",
    "    print('Amount of BP-Pairs extracted: ', all_features.shape)\n",
    "    print('Features Extracted.')\n",
    "    print('-----','\\n')\n",
    "    \n",
    "    if csv:\n",
    "        return all_features_path\n",
    "    else:\n",
    "        return all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def drop_correlation(df, labels, threshold = 0.95, plotcorr = False):\n",
    "    corr = df.loc[:, ~df.columns.isin(labels)].corr()\n",
    "    if plotcorr: \n",
    "        f, ax = plt.subplots(figsize=(15, 15))\n",
    "        cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "        sns.heatmap(corr, cmap = cmap,\n",
    "                xticklabels=corr.columns.values,\n",
    "                yticklabels=corr.columns.values)\n",
    "    # Select upper triangle of correlation matrix\n",
    "    upper = corr.abs().where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "    # Find features with correlation greater than threshold\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "    print(\"Dataframe Shape: \" + str(df.shape))\n",
    "    print(\"Columns dropped: \", len(to_drop))\n",
    "    # Drop features \n",
    "    new_df = df.drop(columns = to_drop)\n",
    "    print(\"New Dataframe Shape: \" + str(new_df.shape))\n",
    "    return(new_df)\n",
    "\n",
    "def predict_bp_from_ppg(dataframe, predicted_variable = 'SBP', k = 1, correlation_threshold = 0.95, \n",
    "                        random_seed = 42, learning_rate = 0.01, n_estimators = 100, \n",
    "                        alpha = 1, l1_ratio = 0.5, random_state = 42, \n",
    "                        epochs = 50, batch_size = 5, n_jobs = -1, max_depth = 10, verbose = False):\n",
    "    \n",
    "    df = dataframe.rename(columns={\"SYS(mmHg)\": \"SBP\", \"DIA(mmHg)\": \"DBP\", 'subject': 'patientid'})\n",
    "    cols_dropped = ['timestamp', 'date', 'time', 'window_end', 'window_start']\n",
    "    if verbose: print(\"Cols Dropped: \", cols_dropped)\n",
    "    df.drop(cols_dropped, axis=1, inplace=True)\n",
    "    \n",
    "    # Dropping Correlation\n",
    "    df.drop(df.loc[(df['SBP'] == 0)|(df['DBP'] == 0)].index, inplace = True)\n",
    "    df = drop_correlation(df, ['SBP', 'DBP'], correlation_threshold, plotcorr = False)\n",
    "    if verbose: print(df.shape)\n",
    "\n",
    "    input_shape = df.shape[1]-3\n",
    "    print('Nr of features: ', input_shape)\n",
    "    patient_ids = np.unique(df['patientid'])\n",
    "\n",
    "    estimators_lr = []\n",
    "    estimators_lr.append(('standardize', StandardScaler()))\n",
    "    estimators_lr.append(('lr', ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=random_state)))\n",
    "    pipeline_lr = Pipeline(estimators_lr)\n",
    "\n",
    "    estimators_gbm = []\n",
    "    estimators_gbm.append(('standardize', StandardScaler()))\n",
    "    estimators_gbm.append(('gbm', GradientBoostingRegressor(learning_rate=learning_rate, n_estimators=n_estimators, random_state=random_seed)))\n",
    "    pipeline_gbm = Pipeline(estimators_gbm)\n",
    "    \n",
    "    estimators_lgbm = []\n",
    "    estimators_lgbm.append(('standardize', StandardScaler()))\n",
    "    estimators_lgbm.append(('lgbm', lgb.LGBMRegressor(learning_rate=learning_rate, n_estimators=n_estimators, random_state=random_seed, n_jobs=n_jobs)))\n",
    "    pipeline_lgbm = Pipeline(estimators_lgbm)\n",
    "    \n",
    "    estimators_rf = []\n",
    "    estimators_rf.append(('standardize', StandardScaler()))\n",
    "    estimators_rf.append(('rf', RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, random_state=random_state, n_jobs=n_jobs)))\n",
    "    pipeline_rf = Pipeline(estimators_rf)\n",
    "\n",
    "    RMSE_LR = []\n",
    "    MAPE_LR = []\n",
    "    MAE_LR = []\n",
    "\n",
    "    RMSE_GBM = []\n",
    "    MAPE_GBM = []\n",
    "    MAE_GBM = []\n",
    "    \n",
    "    RMSE_LGBM = []\n",
    "    MAPE_LGBM = []\n",
    "    MAE_LGBM = []\n",
    "    \n",
    "    RMSE_RF = []\n",
    "    MAPE_RF = []\n",
    "    MAE_RF = []\n",
    "        \n",
    "    RMSE_DUMMY = []\n",
    "    MAPE_DUMMY = []\n",
    "    MAE_DUMMY = []\n",
    "    \n",
    "    results = {}\n",
    "    i = 0\n",
    "    mean_train = 0\n",
    "    mean_test = 0\n",
    "    total = len(df.index)\n",
    "    subjects = len(df['patientid'].unique())\n",
    "    \n",
    "    if verbose: print(\"BPPairs: \", total)\n",
    "    if verbose: print(\"Subjects: \", subjects)\n",
    "    if verbose: print(\"\\n\")\n",
    "\n",
    "    while len(patient_ids) > 1:\n",
    "        i= i + 1 \n",
    "\n",
    "        # Random Seed\n",
    "        random.seed(random_seed)\n",
    "\n",
    "        patient_test_ids = random.choices(patient_ids, k = k)\n",
    "        patient_ids = [e for e in patient_ids if e not in patient_test_ids]\n",
    "        df_test = df.loc[df['patientid'].isin(patient_test_ids)].dropna()\n",
    "        df_train = df[~df['patientid'].isin(patient_test_ids)].dropna()\n",
    "        if verbose: print(\"Running fold\" + str(i))\n",
    "        if verbose: print(\"Train: \", df_train.shape)\n",
    "        mean_train += len(df_train.index)\n",
    "        if verbose: print(\"Test: \", df_test.shape)\n",
    "        if verbose: print(\"Total: \", len(df_test.index) + len(df_train.index))\n",
    "        mean_test += len(df_test.index)\n",
    "        if verbose: print(\"\\n\")\n",
    "\n",
    "        cols_dropped = ['patientid']\n",
    "\n",
    "        if predicted_variable == 'SBP':\n",
    "            cols_dropped.append('DBP')\n",
    "        elif predicted_variable == 'DBP':\n",
    "            cols_dropped.append('SBP')\n",
    "        df_train = df_train.drop(columns = cols_dropped)\n",
    "        df_test = df_test.drop(columns = cols_dropped)\n",
    "\n",
    "        ##lr\n",
    "        pipeline_lr.fit(X = df_train.loc[:, df_train.columns != predicted_variable].values, \n",
    "                        y = df_train[predicted_variable].values)\n",
    "        predicted_labels = pipeline_lr.predict(df_test.loc[:, df_test.columns != predicted_variable].values)\n",
    "\n",
    "        RMSE_LR.append(np.sqrt(mean_squared_error(df_test[predicted_variable], predicted_labels)))  \n",
    "        MAPE_LR.append(mean_absolute_percentage_error(df_test[predicted_variable], predicted_labels))\n",
    "        MAE_LR.append(mean_absolute_error(df_test[predicted_variable], predicted_labels))\n",
    "\n",
    "        #gbm \n",
    "        pipeline_gbm.fit(X = df_train.loc[:, df_train.columns != predicted_variable].values, \n",
    "                         y = df_train[predicted_variable].values)\n",
    "        predicted_labels = pipeline_gbm.predict(df_test.loc[:, df_test.columns != predicted_variable].values)\n",
    "\n",
    "        RMSE_GBM.append(np.sqrt(mean_squared_error(df_test[predicted_variable], predicted_labels)))  \n",
    "        MAPE_GBM.append(mean_absolute_percentage_error(df_test[predicted_variable], predicted_labels))\n",
    "        MAE_GBM.append(mean_absolute_error(df_test[predicted_variable], predicted_labels))\n",
    "        \n",
    "        #lightgbm\n",
    "        pipeline_lgbm.fit(X = df_train.loc[:, df_train.columns != predicted_variable].values, y = df_train[predicted_variable].values)\n",
    "        predicted_labels = pipeline_lgbm.predict(df_test.loc[:, df_test.columns != predicted_variable].values)\n",
    "\n",
    "        RMSE_LGBM.append(np.sqrt(mean_squared_error(df_test[predicted_variable], predicted_labels)))  \n",
    "        MAPE_LGBM.append(mean_absolute_percentage_error(df_test[predicted_variable], predicted_labels))\n",
    "        MAE_LGBM.append(mean_absolute_error(df_test[predicted_variable], predicted_labels))\n",
    "        \n",
    "        #rf\n",
    "        pipeline_rf.fit(X = df_train.loc[:, df_train.columns != predicted_variable].values, y = df_train[predicted_variable].values)\n",
    "        predicted_labels = pipeline_rf.predict(df_test.loc[:, df_test.columns != predicted_variable].values)\n",
    "\n",
    "        RMSE_RF.append(np.sqrt(mean_squared_error(df_test[predicted_variable], predicted_labels)))  \n",
    "        MAPE_RF.append(mean_absolute_percentage_error(df_test[predicted_variable], predicted_labels))\n",
    "        MAE_RF.append(mean_absolute_error(df_test[predicted_variable], predicted_labels))\n",
    "        \n",
    "        #dummy_mean\n",
    "        dummy_mean = DummyRegressor(strategy='mean')\n",
    "        dummy_mean.fit(X = df_train.loc[:, df_train.columns != predicted_variable].values, \n",
    "                         y = df_train[predicted_variable].values)\n",
    "        predicted_labels = dummy_mean.predict(df_test.loc[:, df_test.columns != predicted_variable].values)\n",
    "\n",
    "        RMSE_DUMMY.append(np.sqrt(mean_squared_error(df_test[predicted_variable], predicted_labels)))  \n",
    "        MAPE_DUMMY.append(mean_absolute_percentage_error(df_test[predicted_variable], predicted_labels))\n",
    "        MAE_DUMMY.append(mean_absolute_error(df_test[predicted_variable], predicted_labels))\n",
    "    \n",
    "    if i == 0:\n",
    "        print('No folds.')\n",
    "        return 0\n",
    "    \n",
    "    # General Info\n",
    "    results['subjects'] = subjects\n",
    "    results['bp_pairs'] = total\n",
    "    results['folders'] = i\n",
    "    results['mean_train_size'] = round(mean_train/i)\n",
    "    results['mean_test_size'] = round(mean_test/i)\n",
    "    \n",
    "    # Mean LR\n",
    "    results['RMSE_LR_MEAN'] = np.mean(np.array(RMSE_LR))\n",
    "    results['MAPE_LR_MEAN'] = np.mean(np.array(MAPE_LR))\n",
    "    results['MAE_LR_MEAN'] = np.mean(np.array(MAE_LR))\n",
    "            \n",
    "    # STD LR\n",
    "    results['RMSE_LR_STD'] = np.std(np.array(RMSE_LR))\n",
    "    results['MAPE_LR_STD'] = np.std(np.array(MAPE_LR))\n",
    "    results['MAE_LR_STD'] = np.std(np.array(MAE_LR))\n",
    "\n",
    "    # Mean GBM\n",
    "    results['RMSE_GBM_MEAN'] = np.mean(np.array(RMSE_GBM))\n",
    "    results['MAPE_GBM_MEAN'] = np.mean(np.array(MAPE_GBM))\n",
    "    results['MAE_GBM_MEAN'] = np.mean(np.array(MAE_GBM))\n",
    "    \n",
    "    # Std GBM\n",
    "    results['RMSE_GBM_STD'] = np.std(np.array(RMSE_GBM))\n",
    "    results['MAPE_GBM_STD'] = np.std(np.array(MAPE_GBM))\n",
    "    results['MAE_GBM_STD'] = np.std(np.array(MAE_GBM))\n",
    "    \n",
    "    # Mean LGBM\n",
    "    results['RMSE_LGBM_MEAN'] = np.mean(np.array(RMSE_LGBM))\n",
    "    results['MAPE_LGBM_MEAN'] = np.mean(np.array(MAPE_LGBM))\n",
    "    results['MAE_LGBM_MEAN'] = np.mean(np.array(MAE_LGBM))\n",
    "    \n",
    "    # Std LGBM\n",
    "    results['RMSE_LGBM_STD'] = np.std(np.array(RMSE_LGBM))\n",
    "    results['MAPE_LGBM_STD'] = np.std(np.array(MAPE_LGBM))\n",
    "    results['MAE_LGBM_STD'] = np.std(np.array(MAE_LGBM))\n",
    "    \n",
    "    # Mean RF\n",
    "    results['RMSE_RF_MEAN'] = np.mean(np.array(RMSE_RF))\n",
    "    results['MAPE_RF_MEAN'] = np.mean(np.array(MAPE_RF))\n",
    "    results['MAE_RF_MEAN'] = np.mean(np.array(MAE_RF))\n",
    "    \n",
    "    # Std RF\n",
    "    results['RMSE_RF_STD'] = np.std(np.array(RMSE_RF))\n",
    "    results['MAPE_RF_STD'] = np.std(np.array(MAPE_RF))\n",
    "    results['MAE_RF_STD'] = np.std(np.array(MAE_RF))\n",
    "    \n",
    "    # Mean Dummy\n",
    "    results['RMSE_DUMMY_MEAN'] = np.mean(np.array(RMSE_DUMMY))\n",
    "    results['MAPE_DUMMY_MEAN'] = np.mean(np.array(MAPE_DUMMY))\n",
    "    results['MAE_DUMMY_MEAN'] = np.mean(np.array(MAE_DUMMY))\n",
    "    \n",
    "    # Std Dummy\n",
    "    results['RMSE_DUMMY_STD'] = np.std(np.array(RMSE_DUMMY))\n",
    "    results['MAPE_DUMMY_STD'] = np.std(np.array(MAPE_DUMMY))\n",
    "    results['MAE_DUMMY_STD'] = np.std(np.array(MAE_DUMMY))\n",
    "    \n",
    "    parameters = {\n",
    "        'predicted_variable' : predicted_variable,\n",
    "        'correlation_threshold' : correlation_threshold,\n",
    "        'random_seed' :  random_seed,\n",
    "        'learning_rate' : learning_rate, \n",
    "        'n_estimators' : n_estimators, \n",
    "        'alpha' : alpha, \n",
    "        'l1_ratio' : l1_ratio,\n",
    "        'random_state' : random_state, \n",
    "        'k' : k, \n",
    "        'features' : input_shape, \n",
    "        'epochs' : epochs, \n",
    "        'batch_size' : batch_size,\n",
    "        'max_depth' : max_depth,\n",
    "        'n_jobs' : n_jobs,\n",
    "    }    \n",
    "    results.update(parameters)\n",
    "                         \n",
    "    if verbose: print(\"\\n\")\n",
    "    if verbose: print(\"Number of folds: \", results['folders'])\n",
    "    if verbose: print(\"Mean train size: \", results['mean_train_size'])\n",
    "    if verbose: print(\"Mean test size: \", results['mean_test_size'])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_parameters = {\n",
    "                        'csv' : True,\n",
    "                        'bp_monitor' : 'spacelabs', \n",
    "                        'timeshift' : 2,\n",
    "                        'verbose' : False,\n",
    "                     }\n",
    "\n",
    "time_deltas = ['15 seconds', '30 seconds', '45 seconds']\n",
    "time_delta_types = ['bffill', 'bfill']\n",
    "experiment_types = ['biking', '24 Hours']\n",
    "motion_filters = [True, False]\n",
    "special_filters = ['norm', 'butter', 'cheby']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_paths = []\n",
    "for time_delta_type in time_delta_types:\n",
    "    for time_delta in time_deltas:\n",
    "        for experiment_type in experiment_types:\n",
    "            for motion_filter in motion_filters:\n",
    "                for special_filter in special_filters:\n",
    "                    feature_parameters_variable = {\n",
    "                                            'time_delta' : time_delta, \n",
    "                                            'time_delta_type' : time_delta_type,\n",
    "                                            'experiment_type' : experiment_type, \n",
    "                                            'motion_filter' : motion_filter,\n",
    "                                            'special_filter' : special_filter\n",
    "                                            }\n",
    "                \n",
    "                    feature_parameters.update(feature_parameters_variable)\n",
    "\n",
    "                    path = extract_features(feature_parameters['csv'], feature_parameters['bp_monitor'], feature_parameters['timeshift'],\n",
    "                                  feature_parameters['time_delta'], feature_parameters['time_delta_type'], feature_parameters['experiment_type'], \n",
    "                                  feature_parameters['motion_filter'], feature_parameters['special_filter'], feature_parameters['verbose']\n",
    "                                 )\n",
    "\n",
    "                    all_paths.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_paths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_paths = []\n",
    "base = 'hype'\n",
    "date = datetime.datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "for time_delta_type in time_delta_types:\n",
    "    for time_delta in time_deltas:\n",
    "        for experiment_type in experiment_types:\n",
    "            for special_filter in special_filters:\n",
    "                for motion_filter in motion_filters:\n",
    "                        if motion_filter:\n",
    "                            motion_path_name = 'motion-filtered'\n",
    "                        else:\n",
    "                            motion_path_name = 'motion-not-filtered'\n",
    "                        features_path = os.path.join('..', '..', 'features', base, date, experiment_type.replace(' ',''), time_delta.replace(' ','')+'-'+time_delta_type+'-'+str(special_filter).lower())\n",
    "                        motion_path = os.path.join(features_path, motion_path_name)\n",
    "                        path = os.path.join(motion_path,'all_features_{}_{}.csv'.format(experiment_type.replace(' ',''), time_delta.replace(' ','')+'-'+time_delta_type+'-'+str(special_filter).lower()))\n",
    "                        all_paths.append(path)\n",
    "print(len(all_paths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = []\n",
    "\n",
    "for path in all_paths:\n",
    "    print(\"Predicting for: \", path)\n",
    "    df = pd.read_csv(path)\n",
    "    print(df.shape)\n",
    "    if not df.empty:\n",
    "        predicted_variables = ['SBP', 'DBP']\n",
    "        ks = [1, 2, 3]\n",
    "        \n",
    "        correlation_threshold = 0.75\n",
    "        if df['experiment_type'].unique()[0] == 'biking':\n",
    "            correlation_threshold = 0.6\n",
    "        \n",
    "        features = {\n",
    "                    'time_delta' : df['time_delta'].unique()[0], \n",
    "                    'time_delta_type' : df['time_delta_type'].unique()[0],\n",
    "                    'experiment_type' : df['experiment_type'].unique()[0], \n",
    "                    'motion_filter' : df['motion_filter'].unique()[0],\n",
    "                    'bp_monitor' : df['bp_monitor'].unique()[0], \n",
    "                    'timeshift' : df['timeshift'].unique()[0],\n",
    "                    'special_filter' : df['special_filter'].unique()[0]\n",
    "                    }\n",
    "        \n",
    "        df.drop(features.keys(), axis=1, inplace=True)\n",
    "        for variable in predicted_variables:\n",
    "            for k in ks:\n",
    "                results = predict_bp_from_ppg(df, predicted_variable = variable, \n",
    "                                              k = k, correlation_threshold = correlation_threshold\n",
    "                                             )\n",
    "                if results != 0:\n",
    "                    results.update(features)\n",
    "                    experiments.append(results)\n",
    "    else:\n",
    "        (\"Dataframe was empty: \", path)\n",
    "all_experiments = pd.DataFrame.from_dict(experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_experiments = all_experiments.replace({'motion_filter': {True : 'yes', False: 'no'}})\n",
    "all_experiments.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = datetime.datetime.today().strftime('%Y-%m-%d')\n",
    "all_experiments.to_csv('../../results/'+date+'_results_hype.csv', index=True, mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Experiments From Results File and Exploring the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = datetime.datetime.today().strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = '../../results/'+date+'_results_hype.csv'\n",
    "all_experiments = pd.read_csv(results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_experiments.boxplot(column=['MAE_GBM_MEAN'], by='predicted_variable', figsize=(15,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_experiments.boxplot(column=['MAE_DUMMY_MEAN'], by='predicted_variable', figsize=(15,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_experiments.boxplot(column=['MAE_LGBM_MEAN'], by='predicted_variable', figsize=(15,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_experiments.boxplot(column=['MAE_LR_MEAN'], by='predicted_variable', figsize=(15,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_experiments.boxplot(column=['MAE_RF_MEAN'], by='predicted_variable', figsize=(15,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm = all_experiments[['predicted_variable','experiment_type','MAE_GBM_MEAN','MAE_GBM_STD','MAPE_GBM_MEAN','RMSE_GBM_MEAN','special_filter','time_delta','time_delta_type','motion_filter','k']].sort_values(by=['predicted_variable','experiment_type','k','MAE_GBM_MEAN'])\n",
    "lr = all_experiments[['predicted_variable','experiment_type','MAE_LR_MEAN','MAE_LR_STD','MAPE_LR_MEAN','RMSE_LR_MEAN','special_filter','time_delta','time_delta_type','motion_filter','k']].sort_values(by=['predicted_variable','experiment_type','k','MAE_LR_MEAN'])\n",
    "lgbm = all_experiments[['predicted_variable','experiment_type','MAE_LGBM_MEAN','MAE_LGBM_STD','MAPE_LGBM_MEAN','RMSE_LGBM_MEAN','special_filter','time_delta','time_delta_type','motion_filter','k']].sort_values(by=['predicted_variable','experiment_type','k','MAE_LGBM_MEAN'])\n",
    "rf = all_experiments[['predicted_variable','experiment_type','MAE_RF_MEAN','MAE_RF_STD','MAPE_RF_MEAN','RMSE_RF_MEAN','special_filter','time_delta','time_delta_type','motion_filter','k']].sort_values(by=['predicted_variable','experiment_type','k','MAE_RF_MEAN'])\n",
    "d = all_experiments[['predicted_variable','experiment_type','MAE_DUMMY_MEAN','MAE_DUMMY_STD','MAPE_DUMMY_MEAN','RMSE_DUMMY_MEAN','special_filter','time_delta','time_delta_type','motion_filter','k']].sort_values(by=['predicted_variable','experiment_type','k','MAE_DUMMY_MEAN'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# min 24h dbp\n",
    "gbm_min_24_dbp = gbm.loc[(gbm['experiment_type'] == '24 Hours') & (gbm['predicted_variable'] == 'DBP') & (gbm['k'] == 1)].head(1)[['predicted_variable','experiment_type','k','MAE_GBM_MEAN','MAE_GBM_STD']]\n",
    "lgbm_min_24_dbp = lgbm.loc[(lgbm['experiment_type'] == '24 Hours') & (lgbm['predicted_variable'] == 'DBP') & (lgbm['k'] == 1)].head(1)[['predicted_variable','experiment_type','k','MAE_LGBM_MEAN','MAE_LGBM_STD']]\n",
    "rf_min_24_dbp = rf.loc[(rf['experiment_type'] == '24 Hours') & (rf['predicted_variable'] == 'DBP') & (rf['k'] == 1)].head(1)[['predicted_variable','experiment_type','k','MAE_RF_MEAN','MAE_RF_STD']]\n",
    "lr_min_24_dbp = lr.loc[(lr['experiment_type'] == '24 Hours') & (lr['predicted_variable'] == 'DBP') & (lr['k'] == 1)].head(1)[['predicted_variable','experiment_type','k','MAE_LR_MEAN','MAE_LR_STD']]\n",
    "dummy_min_24_dbp = d.loc[(d['experiment_type'] == '24 Hours') & (d['predicted_variable'] == 'DBP') & (d['k'] == 1)].head(1)[['predicted_variable','experiment_type','k','MAE_DUMMY_MEAN','MAE_DUMMY_STD']]\n",
    "\n",
    "min_24_dbp = gbm_min_24_dbp.set_index(['predicted_variable','experiment_type','k']).join(lgbm_min_24_dbp.set_index(['predicted_variable','experiment_type','k']), on=['predicted_variable','experiment_type','k'])\n",
    "min_24_dbp = min_24_dbp.join(rf_min_24_dbp.set_index(['predicted_variable','experiment_type','k']), on=['predicted_variable','experiment_type','k'])\n",
    "min_24_dbp = min_24_dbp.join(lr_min_24_dbp.set_index(['predicted_variable','experiment_type','k']), on=['predicted_variable','experiment_type','k'])\n",
    "min_24_dbp = min_24_dbp.join(dummy_min_24_dbp.set_index(['predicted_variable','experiment_type','k']), on=['predicted_variable','experiment_type','k'])\n",
    "\n",
    "# min 24h sbp\n",
    "gbm_min_24_sbp = gbm.loc[(gbm['experiment_type'] == '24 Hours') & (gbm['predicted_variable'] == 'SBP') & (gbm['k'] == 1)].head(1)[['predicted_variable','experiment_type','k','MAE_GBM_MEAN','MAE_GBM_STD']]\n",
    "lgbm_min_24_sbp = lgbm.loc[(lgbm['experiment_type'] == '24 Hours') & (lgbm['predicted_variable'] == 'SBP') & (lgbm['k'] == 1)].head(1)[['predicted_variable','experiment_type','k','MAE_LGBM_MEAN','MAE_LGBM_STD']]\n",
    "rf_min_24_sbp = rf.loc[(rf['experiment_type'] == '24 Hours') & (rf['predicted_variable'] == 'SBP') & (rf['k'] == 1)].head(1)[['predicted_variable','experiment_type','k','MAE_RF_MEAN','MAE_RF_STD']]\n",
    "lr_min_24_sbp = lr.loc[(lr['experiment_type'] == '24 Hours') & (lr['predicted_variable'] == 'SBP') & (lr['k'] == 1)].head(1)[['predicted_variable','experiment_type','k','MAE_LR_MEAN','MAE_LR_STD']]\n",
    "dummy_min_24_sbp = d.loc[(d['experiment_type'] == '24 Hours') & (d['predicted_variable'] == 'SBP') & (d['k'] == 1)].head(1)[['predicted_variable','experiment_type','k','MAE_DUMMY_MEAN','MAE_DUMMY_STD']]\n",
    "\n",
    "min_24_sbp = gbm_min_24_sbp.set_index(['predicted_variable','experiment_type','k']).join(lgbm_min_24_sbp.set_index(['predicted_variable','experiment_type','k']), on=['predicted_variable','experiment_type','k'])\n",
    "min_24_sbp = min_24_sbp.join(rf_min_24_sbp.set_index(['predicted_variable','experiment_type','k']), on=['predicted_variable','experiment_type','k'])\n",
    "min_24_sbp = min_24_sbp.join(lr_min_24_sbp.set_index(['predicted_variable','experiment_type','k']), on=['predicted_variable','experiment_type','k'])\n",
    "min_24_sbp = min_24_sbp.join(dummy_min_24_sbp.set_index(['predicted_variable','experiment_type','k']), on=['predicted_variable','experiment_type','k'])\n",
    "\n",
    "# min biking dbp\n",
    "gbm_min_biking_dbp = gbm.loc[(gbm['experiment_type'] == 'biking') & (gbm['predicted_variable'] == 'DBP') & (gbm['k'] == 1)].head(1)[['predicted_variable','experiment_type','k','MAE_GBM_MEAN','MAE_GBM_STD']]\n",
    "lgbm_min_biking_dbp = lgbm.loc[(lgbm['experiment_type'] == 'biking') & (lgbm['predicted_variable'] == 'DBP') & (lgbm['k'] == 1)].head(1)[['predicted_variable','experiment_type','k','MAE_LGBM_MEAN','MAE_LGBM_STD']]\n",
    "rf_min_biking_dbp = rf.loc[(rf['experiment_type'] == 'biking') & (rf['predicted_variable'] == 'DBP') & (rf['k'] == 1)].head(1)[['predicted_variable','experiment_type','k','MAE_RF_MEAN','MAE_RF_STD']]\n",
    "lr_min_biking_dbp = lr.loc[(lr['experiment_type'] == 'biking') & (lr['predicted_variable'] == 'DBP') & (lr['k'] == 1)].head(1)[['predicted_variable','experiment_type','k','MAE_LR_MEAN','MAE_LR_STD']]\n",
    "dummy_min_biking_dbp = d.loc[(d['experiment_type'] == 'biking') & (d['predicted_variable'] == 'DBP') & (d['k'] == 1)].head(1)[['predicted_variable','experiment_type','k','MAE_DUMMY_MEAN','MAE_DUMMY_STD']]\n",
    "\n",
    "min_biking_dbp = gbm_min_biking_dbp.set_index(['predicted_variable','experiment_type','k']).join(lgbm_min_biking_dbp.set_index(['predicted_variable','experiment_type','k']), on=['predicted_variable','experiment_type','k'])\n",
    "min_biking_dbp = min_biking_dbp.join(rf_min_biking_dbp.set_index(['predicted_variable','experiment_type','k']), on=['predicted_variable','experiment_type','k'])\n",
    "min_biking_dbp = min_biking_dbp.join(lr_min_biking_dbp.set_index(['predicted_variable','experiment_type','k']), on=['predicted_variable','experiment_type','k'])\n",
    "min_biking_dbp = min_biking_dbp.join(dummy_min_biking_dbp.set_index(['predicted_variable','experiment_type','k']), on=['predicted_variable','experiment_type','k'])\n",
    "\n",
    "# min biking sbp\n",
    "gbm_min_biking_sbp = gbm.loc[(gbm['experiment_type'] == 'biking') & (gbm['predicted_variable'] == 'SBP') & (gbm['k'] == 1)].head(1)[['predicted_variable','experiment_type','k','MAE_GBM_MEAN','MAE_GBM_STD']]\n",
    "lgbm_min_biking_sbp = lgbm.loc[(lgbm['experiment_type'] == 'biking') & (lgbm['predicted_variable'] == 'SBP') & (lgbm['k'] == 1)].head(1)[['predicted_variable','experiment_type','k','MAE_LGBM_MEAN','MAE_LGBM_STD']]\n",
    "rf_min_biking_sbp = rf.loc[(rf['experiment_type'] == 'biking') & (rf['predicted_variable'] == 'SBP') & (rf['k'] == 1)].head(1)[['predicted_variable','experiment_type','k','MAE_RF_MEAN','MAE_RF_STD']]\n",
    "lr_min_biking_sbp = lr.loc[(lr['experiment_type'] == 'biking') & (lr['predicted_variable'] == 'SBP') & (lr['k'] == 1)].head(1)[['predicted_variable','experiment_type','k','MAE_LR_MEAN','MAE_LR_STD']]\n",
    "dummy_min_biking_sbp = d.loc[(d['experiment_type'] == 'biking') & (d['predicted_variable'] == 'SBP') & (d['k'] == 1)].head(1)[['predicted_variable','experiment_type','k','MAE_DUMMY_MEAN','MAE_DUMMY_STD']]\n",
    "\n",
    "min_biking_sbp = gbm_min_biking_sbp.set_index(['predicted_variable','experiment_type','k']).join(lgbm_min_biking_sbp.set_index(['predicted_variable','experiment_type','k']), on=['predicted_variable','experiment_type','k'])\n",
    "min_biking_sbp = min_biking_sbp.join(rf_min_biking_sbp.set_index(['predicted_variable','experiment_type','k']), on=['predicted_variable','experiment_type','k'])\n",
    "min_biking_sbp = min_biking_sbp.join(lr_min_biking_sbp.set_index(['predicted_variable','experiment_type','k']), on=['predicted_variable','experiment_type','k'])\n",
    "min_biking_sbp = min_biking_sbp.join(dummy_min_biking_sbp.set_index(['predicted_variable','experiment_type','k']), on=['predicted_variable','experiment_type','k'])\n",
    "\n",
    "best_results = pd.concat([min_biking_dbp, min_24_dbp, min_biking_sbp, min_24_sbp], axis=0)\n",
    "best_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = datetime.datetime.today().strftime('%Y-%m-%d')\n",
    "best_results.to_csv('../../results/'+date+'_best_results_hype.csv', index=True, mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group By Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.groupby(['predicted_variable','experiment_type', 'k']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.groupby(['predicted_variable','experiment_type', 'k']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm.groupby(['predicted_variable','experiment_type', 'k']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm.groupby(['predicted_variable','experiment_type', 'k']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.groupby(['predicted_variable','experiment_type', 'k']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm.groupby(['motion_filter', 'special_filter', 'time_delta', 'time_delta_type', 'k']).mean().sort_values(by=['MAE_GBM_MEAN', 'MAE_GBM_STD', 'MAPE_GBM_MEAN']).head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm.groupby(['motion_filter', 'special_filter', 'time_delta', 'time_delta_type', 'k']).mean().sort_values(by=['MAE_LGBM_MEAN', 'MAE_LGBM_STD', 'MAPE_LGBM_MEAN']).head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.groupby(['motion_filter', 'special_filter', 'time_delta', 'time_delta_type', 'k']).mean().sort_values(by=['MAE_RF_MEAN', 'MAE_RF_STD', 'MAPE_RF_MEAN']).head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.groupby(['motion_filter', 'special_filter', 'time_delta', 'time_delta_type', 'k']).mean().sort_values(by=['MAE_DUMMY_MEAN', 'MAE_DUMMY_STD', 'MAPE_DUMMY_MEAN']).head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.groupby(['motion_filter', 'special_filter', 'time_delta', 'time_delta_type', 'k']).mean().sort_values(by=['MAE_LR_MEAN', 'MAE_LR_STD', 'MAPE_LR_MEAN']).head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../config.json') as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "results = results_path.split('/').pop()[:-4]\n",
    "figure_path = os.path.join(config['figures'], results)\n",
    "\n",
    "if not os.path.exists(figure_path):\n",
    "        os.makedirs(figure_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lgbm_k_1_biking = lgbm.loc[(lgbm['k'] == 1) & (lgbm['experiment_type'] == 'biking')].copy()\n",
    "lgbm_k_1_biking.rename(columns={\"MAE_LGBM_MEAN\": \"MAE (LGBM) - Stress Test\", \n",
    "                               \"time_delta\": \"Time Window\", \"time_delta_type\": \"Time Window Type\",\n",
    "                              \"special_filter\": \"Filters\", \"motion_filter\": \"Motion Removal\"}, inplace=True)\n",
    "\n",
    "\n",
    "boxplot = lgbm_k_1_biking.boxplot(column=['MAE (LGBM) - Stress Test'], by='Time Window', figsize=(15,8))\n",
    "boxplot.figure.savefig(os.path.join(figure_path, 'biking-time-window.png'),\n",
    "                    format='png',\n",
    "                    dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_k_1_24 = lgbm.loc[(lgbm['k'] == 1) & (lgbm['experiment_type'] == '24 Hours')].copy()\n",
    "lgbm_k_1_24.rename(columns={\"MAE_LGBM_MEAN\": \"MAE (LGBM) - 24 Hours\", \n",
    "                            \"time_delta\": \"Time Window\", \"time_delta_type\": \"Time Window Type\",\n",
    "                            \"special_filter\": \"Filters\", \"motion_filter\": \"Motion Removal\"}, inplace=True)\n",
    "\n",
    "boxplot = lgbm_k_1_24.boxplot(column=['MAE (LGBM) - 24 Hours'], by='Time Window', figsize=(15,8))\n",
    "boxplot.figure.savefig(os.path.join(figure_path, '24-time-window.png'),\n",
    "                    format='png',\n",
    "                    dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot = lgbm_k_1_biking.boxplot(column=['MAE (LGBM) - Stress Test'], by='Motion Removal', figsize=(15,8))\n",
    "boxplot.figure.savefig(os.path.join(figure_path, 'biking-motion.png'),\n",
    "                    format='png',\n",
    "                    dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot = lgbm_k_1_24.boxplot(column=['MAE (LGBM) - 24 Hours'], by='Motion Removal', figsize=(15,8))\n",
    "boxplot.figure.savefig(os.path.join(figure_path,'24-motion.png'),\n",
    "                    format='png',\n",
    "                    dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot = lgbm_k_1_biking.boxplot(column=['MAE (LGBM) - Stress Test'], by='Filters', figsize=(15,8))\n",
    "boxplot.figure.savefig(os.path.join(figure_path,'biking-filters.png'),\n",
    "                    format='png',\n",
    "                    dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot = lgbm_k_1_24.boxplot(column=['MAE (LGBM) - 24 Hours'], by='Filters', figsize=(15,8))\n",
    "boxplot.figure.savefig(os.path.join(figure_path,'24-filters.png'),\n",
    "                    format='png',\n",
    "                    dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot = lgbm_k_1_biking.boxplot(column=['MAE (LGBM) - Stress Test'], by='Time Window Type', figsize=(15,8))\n",
    "boxplot.figure.savefig(os.path.join(figure_path,'biking-window-type.png'),\n",
    "                    format='png',\n",
    "                    dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "boxplot = lgbm_k_1_24.boxplot(column=['MAE (LGBM) - 24 Hours'], by='Time Window Type', figsize=(15,8))\n",
    "boxplot.figure.savefig(os.path.join(figure_path,'24-window-type.png'),\n",
    "                    format='png',\n",
    "                    dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_parameters = {\n",
    "                        'csv' : False,\n",
    "                        'bp_monitor' : 'spacelabs',\n",
    "                        'timeshift' : 2,\n",
    "                        'time_delta' : '15 seconds',\n",
    "                        'time_delta_type': 'bfill',   \n",
    "                        'experiment_type' : 'biking',\n",
    "                        'motion_filter' : False,\n",
    "                        'special_filter' : 'cheby',\n",
    "                        'verbose' : False\n",
    "                        }\n",
    "\n",
    "df = extract_features(feature_parameters['csv'], feature_parameters['bp_monitor'], feature_parameters['timeshift'],\n",
    "                      feature_parameters['time_delta'], feature_parameters['time_delta_type'], feature_parameters['experiment_type'], \n",
    "                      feature_parameters['motion_filter'], feature_parameters['special_filter'], verbose=feature_parameters['verbose']\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_experiment = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_experiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_experiment = df.drop(columns=['bp_monitor', 'timeshift', 'time_delta', 'time_delta_type', \n",
    "                                 'experiment_type', 'motion_filter', 'special_filter'])\n",
    "experiments = pd.DataFrame()\n",
    "\n",
    "results = predict_bp_from_ppg(df_experiment, predicted_variable='SBP', k=1)\n",
    "results.update(feature_parameters)\n",
    "experiments = experiments.append(results, ignore_index=True)\n",
    "experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_experiment = df.drop(columns=['bp_monitor', 'timeshift', 'time_delta', 'time_delta_type', \n",
    "                                 'experiment_type', 'motion_filter', 'special_filter'])\n",
    "experiments = pd.DataFrame()\n",
    "\n",
    "results = predict_bp_from_ppg(df_experiment, predicted_variable='DBP', k=1)\n",
    "results.update(feature_parameters)\n",
    "experiments = experiments.append(results, ignore_index=True)\n",
    "experiments"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
